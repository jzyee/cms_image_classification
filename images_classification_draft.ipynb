{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "json_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI5RDzjpY4fy",
        "colab_type": "text"
      },
      "source": [
        "#notebook setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8C2C8pRAMSr",
        "colab_type": "code",
        "outputId": "0f16b27a-51e8-47fb-a2b8-10fcec262271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "!pip install google-auth --upgrade\n",
        "!pip install grpcio --upgrade "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: google-auth in /usr/local/lib/python3.6/dist-packages (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth) (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.4.8)\n",
            "Requirement already up-to-date: grpcio in /usr/local/lib/python3.6/dist-packages (1.26.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH7ZObaXX2it",
        "colab_type": "code",
        "outputId": "5731544f-1606-49ec-9032-c5a3809af146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!pip install -qq tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 395.5MB 38kB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 47.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 62.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C7NHc2JYy9U",
        "colab_type": "code",
        "outputId": "24dd3c6e-cf7e-46aa-dc3a-c3fc17eaa154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip freeze"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "absl-py==0.8.1\n",
            "alabaster==0.7.12\n",
            "albumentations==0.1.12\n",
            "altair==4.0.0\n",
            "asgiref==3.2.3\n",
            "astor==0.8.1\n",
            "astropy==3.0.5\n",
            "atari-py==0.2.6\n",
            "atomicwrites==1.3.0\n",
            "attrs==19.3.0\n",
            "audioread==2.1.8\n",
            "autograd==1.3\n",
            "Babel==2.7.0\n",
            "backcall==0.1.0\n",
            "backports.tempfile==1.0\n",
            "backports.weakref==1.0.post1\n",
            "beautifulsoup4==4.6.3\n",
            "bleach==3.1.0\n",
            "blis==0.2.4\n",
            "bokeh==1.0.4\n",
            "boto==2.49.0\n",
            "boto3==1.10.40\n",
            "botocore==1.13.40\n",
            "Bottleneck==1.3.1\n",
            "branca==0.3.1\n",
            "bs4==0.0.1\n",
            "bz2file==0.98\n",
            "cachetools==4.0.0\n",
            "certifi==2019.11.28\n",
            "cffi==1.13.2\n",
            "chainer==6.5.0\n",
            "chardet==3.0.4\n",
            "chart-studio==1.0.0\n",
            "Click==7.0\n",
            "cloudpickle==1.2.2\n",
            "cmake==3.12.0\n",
            "colorlover==0.3.0\n",
            "community==1.0.0b1\n",
            "contextlib2==0.5.5\n",
            "convertdate==2.2.0\n",
            "coverage==3.7.1\n",
            "coveralls==0.5\n",
            "crcmod==1.7\n",
            "cufflinks==0.17.0\n",
            "cupy-cuda101==6.5.0\n",
            "cvxopt==1.2.3\n",
            "cvxpy==1.0.25\n",
            "cycler==0.10.0\n",
            "cymem==2.0.3\n",
            "Cython==0.29.14\n",
            "daft==0.0.4\n",
            "dask==1.1.5\n",
            "dataclasses==0.7\n",
            "datascience==0.10.6\n",
            "decorator==4.4.1\n",
            "defusedxml==0.6.0\n",
            "descartes==1.1.0\n",
            "dill==0.3.1.1\n",
            "distributed==1.25.3\n",
            "Django==3.0\n",
            "dlib==19.18.0\n",
            "dm-sonnet==1.35\n",
            "docopt==0.6.2\n",
            "docutils==0.15.2\n",
            "dopamine-rl==1.0.5\n",
            "earthengine-api==0.1.208\n",
            "easydict==1.9\n",
            "ecos==2.0.7.post1\n",
            "editdistance==0.5.3\n",
            "en-core-web-sm==2.1.0\n",
            "entrypoints==0.3\n",
            "et-xmlfile==1.0.1\n",
            "fa2==0.3.5\n",
            "fancyimpute==0.4.3\n",
            "fastai==1.0.59\n",
            "fastcache==1.1.0\n",
            "fastdtw==0.3.4\n",
            "fastprogress==0.1.22\n",
            "fastrlock==0.4\n",
            "fbprophet==0.5\n",
            "feather-format==0.4.0\n",
            "featuretools==0.4.1\n",
            "filelock==3.0.12\n",
            "fix-yahoo-finance==0.0.22\n",
            "Flask==1.1.1\n",
            "folium==0.8.3\n",
            "fsspec==0.6.2\n",
            "future==0.16.0\n",
            "gast==0.2.2\n",
            "GDAL==2.2.2\n",
            "gdown==3.6.4\n",
            "gensim==3.6.0\n",
            "geographiclib==1.50\n",
            "geopy==1.17.0\n",
            "gevent==1.4.0\n",
            "gin-config==0.2.1\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-api-core==1.15.0\n",
            "google-api-python-client==1.7.11\n",
            "google-auth==1.10.0\n",
            "google-auth-httplib2==0.0.3\n",
            "google-auth-oauthlib==0.4.1\n",
            "google-cloud-bigquery==1.21.0\n",
            "google-cloud-core==1.0.3\n",
            "google-cloud-datastore==1.8.0\n",
            "google-cloud-language==1.2.0\n",
            "google-cloud-storage==1.16.2\n",
            "google-cloud-translate==1.5.0\n",
            "google-colab==1.0.0\n",
            "google-pasta==0.1.8\n",
            "google-resumable-media==0.4.1\n",
            "googleapis-common-protos==1.6.0\n",
            "googledrivedownloader==0.4\n",
            "graph-nets==1.0.5\n",
            "graphviz==0.10.1\n",
            "greenlet==0.4.15\n",
            "grpcio==1.26.0\n",
            "gspread==3.0.1\n",
            "gspread-dataframe==3.0.3\n",
            "gunicorn==20.0.4\n",
            "gym==0.15.4\n",
            "h5py==2.8.0\n",
            "HeapDict==1.0.1\n",
            "holidays==0.9.11\n",
            "html5lib==1.0.1\n",
            "httpimport==0.5.18\n",
            "httplib2==0.11.3\n",
            "humanize==0.5.1\n",
            "hyperopt==0.1.2\n",
            "ideep4py==2.0.0.post3\n",
            "idna==2.8\n",
            "image==1.5.27\n",
            "imageio==2.4.1\n",
            "imagesize==1.1.0\n",
            "imbalanced-learn==0.4.3\n",
            "imblearn==0.0\n",
            "imgaug==0.2.9\n",
            "importlib-metadata==1.3.0\n",
            "imutils==0.5.3\n",
            "inflect==2.1.0\n",
            "intel-openmp==2020.0.133\n",
            "intervaltree==2.1.0\n",
            "ipykernel==4.6.1\n",
            "ipython==5.5.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.3.9\n",
            "ipywidgets==7.5.1\n",
            "itsdangerous==1.1.0\n",
            "jax==0.1.52\n",
            "jaxlib==0.1.36\n",
            "jdcal==1.4.1\n",
            "jedi==0.15.1\n",
            "jieba==0.39\n",
            "Jinja2==2.10.3\n",
            "jmespath==0.9.4\n",
            "joblib==0.14.1\n",
            "jpeg4py==0.1.4\n",
            "jsonschema==2.6.0\n",
            "jupyter==1.0.0\n",
            "jupyter-client==5.3.4\n",
            "jupyter-console==5.2.0\n",
            "jupyter-core==4.6.1\n",
            "kaggle==1.5.6\n",
            "kapre==0.1.3.1\n",
            "Keras==2.2.5\n",
            "Keras-Applications==1.0.8\n",
            "Keras-Preprocessing==1.1.0\n",
            "keras-vis==0.4.1\n",
            "kfac==0.2.0\n",
            "kiwisolver==1.1.0\n",
            "knnimpute==0.1.0\n",
            "librosa==0.6.3\n",
            "lightgbm==2.2.3\n",
            "llvmlite==0.30.0\n",
            "lmdb==0.98\n",
            "lucid==0.3.8\n",
            "lunardate==0.2.0\n",
            "lxml==4.2.6\n",
            "magenta==0.3.19\n",
            "Markdown==3.1.1\n",
            "MarkupSafe==1.1.1\n",
            "matplotlib==3.1.2\n",
            "matplotlib-venn==0.11.5\n",
            "mesh-tensorflow==0.1.7\n",
            "mido==1.2.6\n",
            "mir-eval==0.5\n",
            "missingno==0.4.2\n",
            "mistune==0.8.4\n",
            "mizani==0.5.4\n",
            "mkl==2019.0\n",
            "mlxtend==0.14.0\n",
            "more-itertools==8.0.2\n",
            "moviepy==0.2.3.5\n",
            "mpi4py==3.0.3\n",
            "mpmath==1.1.0\n",
            "msgpack==0.5.6\n",
            "multiprocess==0.70.9\n",
            "multitasking==0.0.9\n",
            "murmurhash==1.0.2\n",
            "music21==5.5.0\n",
            "natsort==5.5.0\n",
            "nbconvert==5.6.1\n",
            "nbformat==4.4.0\n",
            "networkx==2.4\n",
            "nibabel==2.3.3\n",
            "nltk==3.2.5\n",
            "notebook==5.2.2\n",
            "np-utils==0.5.12.1\n",
            "numba==0.40.1\n",
            "numexpr==2.7.0\n",
            "numpy==1.17.4\n",
            "nvidia-ml-py3==7.352.0\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.1.0\n",
            "okgrade==0.4.3\n",
            "olefile==0.46\n",
            "opencv-contrib-python==4.1.2.30\n",
            "opencv-python==4.1.2.30\n",
            "openpyxl==2.5.9\n",
            "opt-einsum==3.1.0\n",
            "osqp==0.6.1\n",
            "packaging==19.2\n",
            "palettable==3.3.0\n",
            "pandas==0.25.3\n",
            "pandas-datareader==0.7.4\n",
            "pandas-gbq==0.11.0\n",
            "pandas-profiling==1.4.1\n",
            "pandocfilters==1.4.2\n",
            "parso==0.5.2\n",
            "pathlib==1.0.1\n",
            "patsy==0.5.1\n",
            "pexpect==4.7.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==4.3.0\n",
            "pip-tools==4.2.0\n",
            "plac==0.9.6\n",
            "plotly==4.1.1\n",
            "plotnine==0.5.1\n",
            "pluggy==0.7.1\n",
            "portpicker==1.2.0\n",
            "prefetch-generator==1.0.1\n",
            "preshed==2.0.1\n",
            "pretty-midi==0.2.8\n",
            "prettytable==0.7.2\n",
            "progressbar2==3.38.0\n",
            "prometheus-client==0.7.1\n",
            "promise==2.2.1\n",
            "prompt-toolkit==1.0.18\n",
            "protobuf==3.10.0\n",
            "psutil==5.4.8\n",
            "psycopg2==2.7.6.1\n",
            "ptyprocess==0.6.0\n",
            "py==1.8.0\n",
            "pyarrow==0.14.1\n",
            "pyasn1==0.4.8\n",
            "pyasn1-modules==0.2.7\n",
            "pycocotools==2.0.0\n",
            "pycparser==2.19\n",
            "pydata-google-auth==0.2.1\n",
            "pydot==1.3.0\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "pyemd==0.5.1\n",
            "pyglet==1.3.2\n",
            "Pygments==2.1.3\n",
            "pygobject==3.26.1\n",
            "pymc3==3.7\n",
            "PyMeeus==0.3.6\n",
            "pymongo==3.10.0\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.4\n",
            "pyparsing==2.4.5\n",
            "pypng==0.0.20\n",
            "pyrsistent==0.15.6\n",
            "pysndfile==1.3.8\n",
            "PySocks==1.7.1\n",
            "pystan==2.19.1.1\n",
            "pytest==3.6.4\n",
            "python-apt==1.6.4\n",
            "python-chess==0.23.11\n",
            "python-dateutil==2.6.1\n",
            "python-louvain==0.13\n",
            "python-rtmidi==1.3.1\n",
            "python-slugify==4.0.0\n",
            "python-utils==2.3.0\n",
            "pytz==2018.9\n",
            "PyWavelets==1.1.1\n",
            "PyYAML==3.13\n",
            "pyzmq==17.0.0\n",
            "qtconsole==4.6.0\n",
            "regex==2019.12.9\n",
            "requests==2.21.0\n",
            "requests-oauthlib==1.3.0\n",
            "resampy==0.2.2\n",
            "retrying==1.3.3\n",
            "rpy2==2.9.5\n",
            "rsa==4.0\n",
            "s3fs==0.4.0\n",
            "s3transfer==0.2.1\n",
            "scikit-image==0.15.0\n",
            "scikit-learn==0.21.3\n",
            "scipy==1.3.3\n",
            "screen-resolution-extra==0.0.0\n",
            "scs==2.1.1.post2\n",
            "seaborn==0.9.0\n",
            "semantic-version==2.8.3\n",
            "Send2Trash==1.5.0\n",
            "setuptools-git==1.2\n",
            "Shapely==1.6.4.post2\n",
            "simplegeneric==0.8.1\n",
            "six==1.12.0\n",
            "sklearn==0.0\n",
            "sklearn-pandas==1.8.0\n",
            "smart-open==1.9.0\n",
            "snowballstemmer==2.0.0\n",
            "sortedcontainers==2.1.0\n",
            "spacy==2.1.9\n",
            "Sphinx==1.8.5\n",
            "sphinxcontrib-websupport==1.1.2\n",
            "SQLAlchemy==1.3.12\n",
            "sqlparse==0.3.0\n",
            "srsly==0.2.0\n",
            "stable-baselines==2.2.1\n",
            "statsmodels==0.10.2\n",
            "sympy==1.1.1\n",
            "tables==3.4.4\n",
            "tabulate==0.8.6\n",
            "tb-nightly==2.1.0a20191206\n",
            "tblib==1.6.0\n",
            "tensor2tensor==1.14.1\n",
            "tensorboard==1.15.0\n",
            "tensorboardcolab==0.0.22\n",
            "tensorflow==1.15.0\n",
            "tensorflow-datasets==1.3.2\n",
            "tensorflow-estimator==1.15.1\n",
            "tensorflow-estimator-2.0-preview==2.0.0\n",
            "tensorflow-gan==2.0.0\n",
            "tensorflow-hub==0.7.0\n",
            "tensorflow-metadata==0.15.1\n",
            "tensorflow-privacy==0.2.2\n",
            "tensorflow-probability==0.7.0\n",
            "termcolor==1.1.0\n",
            "terminado==0.8.3\n",
            "testpath==0.4.4\n",
            "text-unidecode==1.3\n",
            "textblob==0.15.3\n",
            "textgenrnn==1.4.1\n",
            "tf-nightly-gpu-2.0-preview==2.0.0.dev20191002\n",
            "tflearn==0.3.2\n",
            "Theano==1.0.4\n",
            "thinc==7.0.8\n",
            "toolz==0.10.0\n",
            "torch==1.3.1\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.3.1\n",
            "torchvision==0.4.2\n",
            "tornado==4.5.3\n",
            "tqdm==4.28.1\n",
            "traitlets==4.3.3\n",
            "tweepy==3.6.0\n",
            "typing==3.6.6\n",
            "typing-extensions==3.6.6\n",
            "tzlocal==1.5.1\n",
            "umap-learn==0.3.10\n",
            "uritemplate==3.0.0\n",
            "urllib3==1.24.3\n",
            "vega-datasets==0.7.0\n",
            "wasabi==0.4.2\n",
            "wcwidth==0.1.7\n",
            "webencodings==0.5.1\n",
            "Werkzeug==0.16.0\n",
            "widgetsnbextension==3.5.1\n",
            "wordcloud==1.5.0\n",
            "wrapt==1.11.2\n",
            "xarray==0.11.3\n",
            "xgboost==0.90\n",
            "xkit==0.0.0\n",
            "xlrd==1.1.0\n",
            "xlwt==1.3.0\n",
            "yellowbrick==0.9.1\n",
            "zict==1.0.0\n",
            "zipp==0.6.0\n",
            "zmq==0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIXnBwudYLjl",
        "colab_type": "code",
        "outputId": "0e942dc5-a7ef-4838-91df-2e28eebbc58a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3FhXNV5u0NI",
        "colab_type": "code",
        "outputId": "6b10b36c-c953-4e8f-97bf-efb7b4582d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-dev20191002'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixmPfjVmYYQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "try:\n",
        "    import ujson as json\n",
        "except ImportError:\n",
        "    try:\n",
        "        import simplejson as json\n",
        "    except ImportError:\n",
        "        import json\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pywt\n",
        "import os\n",
        "from PIL import Image\n",
        "# import tensorflow as tf\n",
        "from glob import glob\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.models import Model,load_model,Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout,Flatten, Input\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u0wv_jGZF-L",
        "colab_type": "code",
        "outputId": "39b8c5fb-35ef-4e24-bf96-54ba97f2b594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQKsdJvdUiDn",
        "colab_type": "code",
        "outputId": "91dfb52e-cec8-48da-a1d7-6f6f668b9155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jan  7 20:30:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    32W / 250W |    265MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Pb6W4AZa0d",
        "colab_type": "text"
      },
      "source": [
        "# Data prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRZpm9JxZKEN",
        "colab_type": "code",
        "outputId": "c79c64c7-e465-4167-8b26-c1159e293f57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%cd drive/'My Drive'\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1MEckdzZ_-1",
        "colab_type": "code",
        "outputId": "e36afbcf-d9d4-4508-f338-24b096aa1d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "img_folder = '/Images'\n",
        "!ls 'Images'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  1  2  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSJJ8GzogU_i",
        "colab_type": "text"
      },
      "source": [
        "class 0 : DYjets\n",
        "class 1 : TTjets\n",
        "class 2 : Wjets\n",
        "class 3 : ZZjets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK3uFq83Nqp7",
        "colab_type": "text"
      },
      "source": [
        "## Train val size setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd5rd7_ZcoSR",
        "colab_type": "code",
        "outputId": "8ce149c3-f174-414b-d901-65ea89e2efa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "cwd = os.getcwd()\n",
        "img_path = cwd + img_folder\n",
        "\n",
        "train_val_size = 4000\n",
        "\n",
        "df = {}\n",
        "data = []\n",
        "for idx, x in enumerate(os.listdir(img_path)):\n",
        "  if x.isdigit() and idx < 3:\n",
        "    for idx2, pic in enumerate(os.listdir(img_path+'/'+str(x))):\n",
        "      if idx2 < train_val_size:\n",
        "        data.append((img_path+ '/' + x + '/' + pic , x))\n",
        "\n",
        "df = pd.DataFrame(data, columns=['filename', 'class'])\n",
        "df['class'] = df['class'].astype(int)\n",
        "#shuffle\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(df.head())\n",
        "print(len(df))\n",
        "df.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            filename  class\n",
            "0  /content/drive/My Drive/Images/1/TTjets_event_...      1\n",
            "1  /content/drive/My Drive/Images/1/TTjets_event_...      1\n",
            "2  /content/drive/My Drive/Images/2/Wjets_event_3...      2\n",
            "3  /content/drive/My Drive/Images/0/DYjets_event_...      0\n",
            "4  /content/drive/My Drive/Images/0/DYjets_event_...      0\n",
            "12000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "filename    object\n",
              "class        int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF67c9pOdMix",
        "colab_type": "code",
        "outputId": "50666ce5-eded-4c4c-feec-df8da0948b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from collections import Counter\n",
        "cnt = Counter(df['class'])\n",
        "cnt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 4000, 1: 4000, 2: 4000})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV8WCnbQhKx4",
        "colab_type": "code",
        "outputId": "0311db4c-aae8-4778-c171-9410549eeeed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Split into train and validation sets\n",
        "train_percentage = .7\n",
        "val_percentage = 0.15\n",
        "test_percentage = 0.15\n",
        "\n",
        "train_length = int(len(df) * train_percentage)\n",
        "df_train = df[:train_length]\n",
        "\n",
        "val_length = int(len(df) * val_percentage)\n",
        "df_val = df[train_length : train_length+val_length]\n",
        "\n",
        "test_length = int(len(df) * test_percentage)\n",
        "df_test = df[ train_length + val_length : ]\n",
        "\n",
        "# shuffle \n",
        "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
        "df_val = df_val.sample(frac=1).reset_index(drop=True)\n",
        "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "df_list = [df_train, df_val, df_test]\n",
        "for df_part in df_list:\n",
        "  print(len(df_part))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8400\n",
            "1800\n",
            "1800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3BhmpUsNt6u",
        "colab_type": "code",
        "outputId": "cbd5998d-3ca9-4beb-e4a7-eb208dd8654b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# print(df_train)\n",
        "print(set(df['class']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 1, 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93QOO1_2h5_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reads an image from a file, decodes it into a tensor, and resizes it\n",
        "# to a fixed shape.\n",
        "img_rows, img_cols = 224,224\n",
        "num_classes = len(set(df['class']))\n",
        "batch_size = 128\n",
        "\n",
        "def _parse_function(filename, label):\n",
        "  image_string = tf.io.read_file(filename)\n",
        "  image_decoded = tf.image.decode_jpeg(image_string)\n",
        "  image_resized = tf.image.resize(image_decoded, [img_rows, img_cols])\n",
        "  image_resized = tf.ensure_shape(image_resized ,shape=(img_rows, img_cols,3))\n",
        "  label = tf.one_hot(label, num_classes)\n",
        "  return image_resized, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9GwWMh5iZtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(df_train['filename'].values, tf.string),\n",
        "                                                    tf.cast(df_train['class'].values, tf.int32) ))\n",
        "train_dataset = train_dataset.map(_parse_function)\n",
        "train_dataset = train_dataset.shuffle(5000)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MfsNRyiicIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(df_val['filename'].values, tf.string),\n",
        "                                                    tf.cast(df_val['class'].values, tf.int32) ))\n",
        "valid_dataset = valid_dataset.map(_parse_function)\n",
        "valid_dataset = valid_dataset.shuffle(5000)\n",
        "valid_dataset = valid_dataset.repeat()\n",
        "valid_dataset = valid_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amK7DVJDPTSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(df_test['filename'].values, tf.string),\n",
        "                                                    tf.cast(df_test['class'].values, tf.int32) ))\n",
        "test_dataset = test_dataset.map(_parse_function)\n",
        "test_dataset = test_dataset.shuffle(5000)\n",
        "test_dataset = test_dataset.repeat()\n",
        "test_dataset = test_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PsYgoDSieIA",
        "colab_type": "code",
        "outputId": "6de0f46c-d37f-49d7-f648-afb6cb480758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 224, 224, 3), (128, 3)), types: (tf.float32, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1vIZxA0RXwN",
        "colab_type": "code",
        "outputId": "790928be-4c81-4161-c666-3f9f695db887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "valid_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 224, 224, 3), (128, 3)), types: (tf.float32, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBGgnehQComV",
        "colab_type": "text"
      },
      "source": [
        "# inception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmYn28dKigPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_inception = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3), pooling=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c01m47ajGzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add a global spatial average pooling layer\n",
        "x = base_inception.output\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "#x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# and a logits layer -- let's say we have 3 classes\n",
        "predictions = Dense(3, activation='softmax')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yOvde7kjzsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inception_model = Model(inputs=base_inception.input, outputs=predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRQsyK3RK57P",
        "colab_type": "code",
        "outputId": "4f19bf3c-f694-4c15-8376-5296677d4d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inception_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 111, 111, 32) 864         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 111, 111, 32) 96          conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 111, 111, 32) 0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 109, 109, 32) 9216        activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 109, 109, 32) 96          conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 109, 109, 32) 0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 109, 109, 64) 18432       activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 109, 109, 64) 192         conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 109, 109, 64) 0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 54, 54, 64)   0           activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 54, 54, 80)   5120        max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 54, 54, 80)   240         conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 54, 54, 80)   0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 52, 52, 192)  138240      activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 52, 52, 192)  576         conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 52, 52, 192)  0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 25, 25, 64)   192         conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 25, 25, 64)   0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 25, 25, 48)   9216        max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 25, 25, 96)   55296       activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 25, 25, 48)   144         conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 25, 25, 96)   288         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 25, 25, 48)   0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 25, 25, 96)   0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_18 (AveragePo (None, 25, 25, 192)  0           max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 25, 25, 64)   76800       activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 25, 25, 96)   82944       activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 25, 25, 32)   6144        average_pooling2d_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 25, 25, 64)   192         conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 25, 25, 64)   192         conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 25, 25, 96)   288         conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 25, 25, 32)   96          conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 25, 25, 64)   0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 25, 25, 64)   0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 25, 25, 96)   0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 25, 25, 32)   0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_193[0][0]             \n",
            "                                                                 activation_195[0][0]             \n",
            "                                                                 activation_198[0][0]             \n",
            "                                                                 activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 25, 25, 64)   192         conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 25, 25, 64)   0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 25, 25, 96)   55296       activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 25, 25, 48)   144         conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 25, 25, 96)   288         conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 25, 25, 48)   0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 25, 25, 96)   0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_19 (AveragePo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 25, 25, 64)   76800       activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 25, 25, 96)   82944       activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 25, 25, 64)   16384       average_pooling2d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 25, 25, 64)   192         conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 25, 25, 64)   192         conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 25, 25, 96)   288         conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 25, 25, 64)   192         conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 25, 25, 64)   0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 25, 25, 64)   0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 25, 25, 96)   0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 25, 25, 64)   0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_200[0][0]             \n",
            "                                                                 activation_202[0][0]             \n",
            "                                                                 activation_205[0][0]             \n",
            "                                                                 activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 25, 25, 64)   192         conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 25, 25, 64)   0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 25, 25, 96)   55296       activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 25, 25, 48)   144         conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 25, 25, 96)   288         conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 25, 25, 48)   0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 25, 25, 96)   0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_20 (AveragePo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 25, 25, 64)   76800       activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 25, 25, 96)   82944       activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 25, 25, 64)   18432       average_pooling2d_20[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 25, 25, 64)   192         conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 25, 25, 64)   192         conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 25, 25, 96)   288         conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 25, 25, 64)   192         conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 25, 25, 64)   0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 25, 25, 64)   0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 25, 25, 96)   0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 25, 25, 64)   0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_207[0][0]             \n",
            "                                                                 activation_209[0][0]             \n",
            "                                                                 activation_212[0][0]             \n",
            "                                                                 activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 25, 25, 64)   192         conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 25, 25, 64)   0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 25, 25, 96)   55296       activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 25, 25, 96)   288         conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 25, 25, 96)   0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 12, 12, 96)   82944       activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 12, 12, 384)  1152        conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 12, 12, 96)   288         conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 12, 12, 384)  0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 12, 12, 96)   0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_214[0][0]             \n",
            "                                                                 activation_217[0][0]             \n",
            "                                                                 max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 12, 12, 128)  384         conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 12, 12, 128)  0           batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 12, 12, 128)  114688      activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 12, 12, 128)  384         conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 12, 12, 128)  0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 12, 12, 128)  114688      activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 12, 12, 128)  384         conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 12, 12, 128)  384         conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 12, 12, 128)  0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 12, 12, 128)  0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 12, 12, 128)  114688      activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 12, 12, 128)  114688      activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 12, 12, 128)  384         conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 12, 12, 128)  384         conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 12, 12, 128)  0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 12, 12, 128)  0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_21 (AveragePo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 12, 12, 192)  172032      activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 12, 12, 192)  172032      activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_21[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 12, 12, 192)  576         conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 12, 12, 192)  576         conv2d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 12, 12, 192)  576         conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 12, 12, 192)  576         conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 12, 12, 192)  0           batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 12, 12, 192)  0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 12, 12, 192)  0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 12, 12, 192)  0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_218[0][0]             \n",
            "                                                                 activation_221[0][0]             \n",
            "                                                                 activation_226[0][0]             \n",
            "                                                                 activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 12, 12, 160)  480         conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 12, 12, 160)  0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 12, 12, 160)  179200      activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 12, 12, 160)  480         conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 12, 12, 160)  0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 12, 12, 160)  179200      activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 12, 12, 160)  480         conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 12, 12, 160)  480         conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 12, 12, 160)  0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 12, 12, 160)  0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 12, 12, 160)  179200      activation_229[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 12, 12, 160)  179200      activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 12, 12, 160)  480         conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 12, 12, 160)  480         conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 12, 12, 160)  0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 12, 12, 160)  0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_22 (AveragePo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 12, 12, 192)  215040      activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 12, 12, 192)  215040      activation_235[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_22[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 12, 12, 192)  576         conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 12, 12, 192)  576         conv2d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 12, 12, 192)  576         conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 12, 12, 192)  576         conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 12, 12, 192)  0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 12, 12, 192)  0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 12, 12, 192)  0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 12, 12, 192)  0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_228[0][0]             \n",
            "                                                                 activation_231[0][0]             \n",
            "                                                                 activation_236[0][0]             \n",
            "                                                                 activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_242 (BatchN (None, 12, 12, 160)  480         conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 12, 12, 160)  0           batch_normalization_242[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 12, 12, 160)  179200      activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_243 (BatchN (None, 12, 12, 160)  480         conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 12, 12, 160)  0           batch_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 12, 12, 160)  179200      activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 12, 12, 160)  480         conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_244 (BatchN (None, 12, 12, 160)  480         conv2d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 12, 12, 160)  0           batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 12, 12, 160)  0           batch_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 12, 12, 160)  179200      activation_239[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 12, 12, 160)  179200      activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_240 (BatchN (None, 12, 12, 160)  480         conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_245 (BatchN (None, 12, 12, 160)  480         conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 12, 12, 160)  0           batch_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 12, 12, 160)  0           batch_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_23 (AveragePo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 12, 12, 192)  215040      activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 12, 12, 192)  215040      activation_245[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_23[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 12, 12, 192)  576         conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_241 (BatchN (None, 12, 12, 192)  576         conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_246 (BatchN (None, 12, 12, 192)  576         conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_247 (BatchN (None, 12, 12, 192)  576         conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 12, 12, 192)  0           batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 12, 12, 192)  0           batch_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 12, 12, 192)  0           batch_normalization_246[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_247 (Activation)     (None, 12, 12, 192)  0           batch_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_238[0][0]             \n",
            "                                                                 activation_241[0][0]             \n",
            "                                                                 activation_246[0][0]             \n",
            "                                                                 activation_247[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_252 (BatchN (None, 12, 12, 192)  576         conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_252 (Activation)     (None, 12, 12, 192)  0           batch_normalization_252[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 12, 12, 192)  258048      activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_253 (BatchN (None, 12, 12, 192)  576         conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_253 (Activation)     (None, 12, 12, 192)  0           batch_normalization_253[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 12, 12, 192)  258048      activation_253[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_249 (BatchN (None, 12, 12, 192)  576         conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_254 (BatchN (None, 12, 12, 192)  576         conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_249 (Activation)     (None, 12, 12, 192)  0           batch_normalization_249[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_254 (Activation)     (None, 12, 12, 192)  0           batch_normalization_254[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 12, 12, 192)  258048      activation_249[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 12, 12, 192)  258048      activation_254[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_250 (BatchN (None, 12, 12, 192)  576         conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_255 (BatchN (None, 12, 12, 192)  576         conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_250 (Activation)     (None, 12, 12, 192)  0           batch_normalization_250[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_255 (Activation)     (None, 12, 12, 192)  0           batch_normalization_255[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_24 (AveragePo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 12, 12, 192)  258048      activation_250[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_256 (Conv2D)             (None, 12, 12, 192)  258048      activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_257 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_24[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_248 (BatchN (None, 12, 12, 192)  576         conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_251 (BatchN (None, 12, 12, 192)  576         conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_256 (BatchN (None, 12, 12, 192)  576         conv2d_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_257 (BatchN (None, 12, 12, 192)  576         conv2d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_248 (Activation)     (None, 12, 12, 192)  0           batch_normalization_248[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_251 (Activation)     (None, 12, 12, 192)  0           batch_normalization_251[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_256 (Activation)     (None, 12, 12, 192)  0           batch_normalization_256[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_257 (Activation)     (None, 12, 12, 192)  0           batch_normalization_257[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_248[0][0]             \n",
            "                                                                 activation_251[0][0]             \n",
            "                                                                 activation_256[0][0]             \n",
            "                                                                 activation_257[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_260 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_260 (BatchN (None, 12, 12, 192)  576         conv2d_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_260 (Activation)     (None, 12, 12, 192)  0           batch_normalization_260[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_261 (Conv2D)             (None, 12, 12, 192)  258048      activation_260[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_261 (BatchN (None, 12, 12, 192)  576         conv2d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_261 (Activation)     (None, 12, 12, 192)  0           batch_normalization_261[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_258 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_262 (Conv2D)             (None, 12, 12, 192)  258048      activation_261[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_258 (BatchN (None, 12, 12, 192)  576         conv2d_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_262 (BatchN (None, 12, 12, 192)  576         conv2d_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_258 (Activation)     (None, 12, 12, 192)  0           batch_normalization_258[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_262 (Activation)     (None, 12, 12, 192)  0           batch_normalization_262[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_259 (Conv2D)             (None, 5, 5, 320)    552960      activation_258[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_263 (Conv2D)             (None, 5, 5, 192)    331776      activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_259 (BatchN (None, 5, 5, 320)    960         conv2d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_263 (BatchN (None, 5, 5, 192)    576         conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_259 (Activation)     (None, 5, 5, 320)    0           batch_normalization_259[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_263 (Activation)     (None, 5, 5, 192)    0           batch_normalization_263[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_259[0][0]             \n",
            "                                                                 activation_263[0][0]             \n",
            "                                                                 max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_268 (Conv2D)             (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_268 (BatchN (None, 5, 5, 448)    1344        conv2d_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_268 (Activation)     (None, 5, 5, 448)    0           batch_normalization_268[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_265 (Conv2D)             (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_269 (Conv2D)             (None, 5, 5, 384)    1548288     activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_265 (BatchN (None, 5, 5, 384)    1152        conv2d_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_269 (BatchN (None, 5, 5, 384)    1152        conv2d_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_265 (Activation)     (None, 5, 5, 384)    0           batch_normalization_265[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_269 (Activation)     (None, 5, 5, 384)    0           batch_normalization_269[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_266 (Conv2D)             (None, 5, 5, 384)    442368      activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_267 (Conv2D)             (None, 5, 5, 384)    442368      activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_270 (Conv2D)             (None, 5, 5, 384)    442368      activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_271 (Conv2D)             (None, 5, 5, 384)    442368      activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_25 (AveragePo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_264 (Conv2D)             (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_266 (BatchN (None, 5, 5, 384)    1152        conv2d_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_267 (BatchN (None, 5, 5, 384)    1152        conv2d_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_270 (BatchN (None, 5, 5, 384)    1152        conv2d_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_271 (BatchN (None, 5, 5, 384)    1152        conv2d_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_272 (Conv2D)             (None, 5, 5, 192)    245760      average_pooling2d_25[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_264 (BatchN (None, 5, 5, 320)    960         conv2d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_266 (Activation)     (None, 5, 5, 384)    0           batch_normalization_266[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_267 (Activation)     (None, 5, 5, 384)    0           batch_normalization_267[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_270 (Activation)     (None, 5, 5, 384)    0           batch_normalization_270[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_271 (Activation)     (None, 5, 5, 384)    0           batch_normalization_271[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_272 (BatchN (None, 5, 5, 192)    576         conv2d_272[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_264 (Activation)     (None, 5, 5, 320)    0           batch_normalization_264[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_266[0][0]             \n",
            "                                                                 activation_267[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 5, 5, 768)    0           activation_270[0][0]             \n",
            "                                                                 activation_271[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_272 (Activation)     (None, 5, 5, 192)    0           batch_normalization_272[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_264[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_4[0][0]              \n",
            "                                                                 activation_272[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_277 (Conv2D)             (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_277 (BatchN (None, 5, 5, 448)    1344        conv2d_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_277 (Activation)     (None, 5, 5, 448)    0           batch_normalization_277[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_274 (Conv2D)             (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_278 (Conv2D)             (None, 5, 5, 384)    1548288     activation_277[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_274 (BatchN (None, 5, 5, 384)    1152        conv2d_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_278 (BatchN (None, 5, 5, 384)    1152        conv2d_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_274 (Activation)     (None, 5, 5, 384)    0           batch_normalization_274[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_278 (Activation)     (None, 5, 5, 384)    0           batch_normalization_278[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_275 (Conv2D)             (None, 5, 5, 384)    442368      activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_276 (Conv2D)             (None, 5, 5, 384)    442368      activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_279 (Conv2D)             (None, 5, 5, 384)    442368      activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_280 (Conv2D)             (None, 5, 5, 384)    442368      activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_26 (AveragePo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_273 (Conv2D)             (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_275 (BatchN (None, 5, 5, 384)    1152        conv2d_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_276 (BatchN (None, 5, 5, 384)    1152        conv2d_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_279 (BatchN (None, 5, 5, 384)    1152        conv2d_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_280 (BatchN (None, 5, 5, 384)    1152        conv2d_280[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_281 (Conv2D)             (None, 5, 5, 192)    393216      average_pooling2d_26[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_273 (BatchN (None, 5, 5, 320)    960         conv2d_273[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_275 (Activation)     (None, 5, 5, 384)    0           batch_normalization_275[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_276 (Activation)     (None, 5, 5, 384)    0           batch_normalization_276[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_279 (Activation)     (None, 5, 5, 384)    0           batch_normalization_279[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_280 (Activation)     (None, 5, 5, 384)    0           batch_normalization_280[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_281 (BatchN (None, 5, 5, 192)    576         conv2d_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_273 (Activation)     (None, 5, 5, 320)    0           batch_normalization_273[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_275[0][0]             \n",
            "                                                                 activation_276[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 5, 5, 768)    0           activation_279[0][0]             \n",
            "                                                                 activation_280[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_281 (Activation)     (None, 5, 5, 192)    0           batch_normalization_281[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_273[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_5[0][0]              \n",
            "                                                                 activation_281[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          1049088     global_average_pooling2d_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 512)          262656      dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 512)          0           dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 512)          262656      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 512)          0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 3)            1539        dropout_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 23,378,723\n",
            "Trainable params: 23,344,291\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K57xbdt9kdm8",
        "colab_type": "code",
        "outputId": "fd5d4e61-36d9-4799-96bd-6e83a569d834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for layer in base_inception.layers:\n",
        "    layer.trainable = False\n",
        "    print(layer.name, ' Trainable =', layer.trainable)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_7  Trainable = False\n",
            "conv2d_188  Trainable = False\n",
            "batch_normalization_188  Trainable = False\n",
            "activation_188  Trainable = False\n",
            "conv2d_189  Trainable = False\n",
            "batch_normalization_189  Trainable = False\n",
            "activation_189  Trainable = False\n",
            "conv2d_190  Trainable = False\n",
            "batch_normalization_190  Trainable = False\n",
            "activation_190  Trainable = False\n",
            "max_pooling2d_8  Trainable = False\n",
            "conv2d_191  Trainable = False\n",
            "batch_normalization_191  Trainable = False\n",
            "activation_191  Trainable = False\n",
            "conv2d_192  Trainable = False\n",
            "batch_normalization_192  Trainable = False\n",
            "activation_192  Trainable = False\n",
            "max_pooling2d_9  Trainable = False\n",
            "conv2d_196  Trainable = False\n",
            "batch_normalization_196  Trainable = False\n",
            "activation_196  Trainable = False\n",
            "conv2d_194  Trainable = False\n",
            "conv2d_197  Trainable = False\n",
            "batch_normalization_194  Trainable = False\n",
            "batch_normalization_197  Trainable = False\n",
            "activation_194  Trainable = False\n",
            "activation_197  Trainable = False\n",
            "average_pooling2d_18  Trainable = False\n",
            "conv2d_193  Trainable = False\n",
            "conv2d_195  Trainable = False\n",
            "conv2d_198  Trainable = False\n",
            "conv2d_199  Trainable = False\n",
            "batch_normalization_193  Trainable = False\n",
            "batch_normalization_195  Trainable = False\n",
            "batch_normalization_198  Trainable = False\n",
            "batch_normalization_199  Trainable = False\n",
            "activation_193  Trainable = False\n",
            "activation_195  Trainable = False\n",
            "activation_198  Trainable = False\n",
            "activation_199  Trainable = False\n",
            "mixed0  Trainable = False\n",
            "conv2d_203  Trainable = False\n",
            "batch_normalization_203  Trainable = False\n",
            "activation_203  Trainable = False\n",
            "conv2d_201  Trainable = False\n",
            "conv2d_204  Trainable = False\n",
            "batch_normalization_201  Trainable = False\n",
            "batch_normalization_204  Trainable = False\n",
            "activation_201  Trainable = False\n",
            "activation_204  Trainable = False\n",
            "average_pooling2d_19  Trainable = False\n",
            "conv2d_200  Trainable = False\n",
            "conv2d_202  Trainable = False\n",
            "conv2d_205  Trainable = False\n",
            "conv2d_206  Trainable = False\n",
            "batch_normalization_200  Trainable = False\n",
            "batch_normalization_202  Trainable = False\n",
            "batch_normalization_205  Trainable = False\n",
            "batch_normalization_206  Trainable = False\n",
            "activation_200  Trainable = False\n",
            "activation_202  Trainable = False\n",
            "activation_205  Trainable = False\n",
            "activation_206  Trainable = False\n",
            "mixed1  Trainable = False\n",
            "conv2d_210  Trainable = False\n",
            "batch_normalization_210  Trainable = False\n",
            "activation_210  Trainable = False\n",
            "conv2d_208  Trainable = False\n",
            "conv2d_211  Trainable = False\n",
            "batch_normalization_208  Trainable = False\n",
            "batch_normalization_211  Trainable = False\n",
            "activation_208  Trainable = False\n",
            "activation_211  Trainable = False\n",
            "average_pooling2d_20  Trainable = False\n",
            "conv2d_207  Trainable = False\n",
            "conv2d_209  Trainable = False\n",
            "conv2d_212  Trainable = False\n",
            "conv2d_213  Trainable = False\n",
            "batch_normalization_207  Trainable = False\n",
            "batch_normalization_209  Trainable = False\n",
            "batch_normalization_212  Trainable = False\n",
            "batch_normalization_213  Trainable = False\n",
            "activation_207  Trainable = False\n",
            "activation_209  Trainable = False\n",
            "activation_212  Trainable = False\n",
            "activation_213  Trainable = False\n",
            "mixed2  Trainable = False\n",
            "conv2d_215  Trainable = False\n",
            "batch_normalization_215  Trainable = False\n",
            "activation_215  Trainable = False\n",
            "conv2d_216  Trainable = False\n",
            "batch_normalization_216  Trainable = False\n",
            "activation_216  Trainable = False\n",
            "conv2d_214  Trainable = False\n",
            "conv2d_217  Trainable = False\n",
            "batch_normalization_214  Trainable = False\n",
            "batch_normalization_217  Trainable = False\n",
            "activation_214  Trainable = False\n",
            "activation_217  Trainable = False\n",
            "max_pooling2d_10  Trainable = False\n",
            "mixed3  Trainable = False\n",
            "conv2d_222  Trainable = False\n",
            "batch_normalization_222  Trainable = False\n",
            "activation_222  Trainable = False\n",
            "conv2d_223  Trainable = False\n",
            "batch_normalization_223  Trainable = False\n",
            "activation_223  Trainable = False\n",
            "conv2d_219  Trainable = False\n",
            "conv2d_224  Trainable = False\n",
            "batch_normalization_219  Trainable = False\n",
            "batch_normalization_224  Trainable = False\n",
            "activation_219  Trainable = False\n",
            "activation_224  Trainable = False\n",
            "conv2d_220  Trainable = False\n",
            "conv2d_225  Trainable = False\n",
            "batch_normalization_220  Trainable = False\n",
            "batch_normalization_225  Trainable = False\n",
            "activation_220  Trainable = False\n",
            "activation_225  Trainable = False\n",
            "average_pooling2d_21  Trainable = False\n",
            "conv2d_218  Trainable = False\n",
            "conv2d_221  Trainable = False\n",
            "conv2d_226  Trainable = False\n",
            "conv2d_227  Trainable = False\n",
            "batch_normalization_218  Trainable = False\n",
            "batch_normalization_221  Trainable = False\n",
            "batch_normalization_226  Trainable = False\n",
            "batch_normalization_227  Trainable = False\n",
            "activation_218  Trainable = False\n",
            "activation_221  Trainable = False\n",
            "activation_226  Trainable = False\n",
            "activation_227  Trainable = False\n",
            "mixed4  Trainable = False\n",
            "conv2d_232  Trainable = False\n",
            "batch_normalization_232  Trainable = False\n",
            "activation_232  Trainable = False\n",
            "conv2d_233  Trainable = False\n",
            "batch_normalization_233  Trainable = False\n",
            "activation_233  Trainable = False\n",
            "conv2d_229  Trainable = False\n",
            "conv2d_234  Trainable = False\n",
            "batch_normalization_229  Trainable = False\n",
            "batch_normalization_234  Trainable = False\n",
            "activation_229  Trainable = False\n",
            "activation_234  Trainable = False\n",
            "conv2d_230  Trainable = False\n",
            "conv2d_235  Trainable = False\n",
            "batch_normalization_230  Trainable = False\n",
            "batch_normalization_235  Trainable = False\n",
            "activation_230  Trainable = False\n",
            "activation_235  Trainable = False\n",
            "average_pooling2d_22  Trainable = False\n",
            "conv2d_228  Trainable = False\n",
            "conv2d_231  Trainable = False\n",
            "conv2d_236  Trainable = False\n",
            "conv2d_237  Trainable = False\n",
            "batch_normalization_228  Trainable = False\n",
            "batch_normalization_231  Trainable = False\n",
            "batch_normalization_236  Trainable = False\n",
            "batch_normalization_237  Trainable = False\n",
            "activation_228  Trainable = False\n",
            "activation_231  Trainable = False\n",
            "activation_236  Trainable = False\n",
            "activation_237  Trainable = False\n",
            "mixed5  Trainable = False\n",
            "conv2d_242  Trainable = False\n",
            "batch_normalization_242  Trainable = False\n",
            "activation_242  Trainable = False\n",
            "conv2d_243  Trainable = False\n",
            "batch_normalization_243  Trainable = False\n",
            "activation_243  Trainable = False\n",
            "conv2d_239  Trainable = False\n",
            "conv2d_244  Trainable = False\n",
            "batch_normalization_239  Trainable = False\n",
            "batch_normalization_244  Trainable = False\n",
            "activation_239  Trainable = False\n",
            "activation_244  Trainable = False\n",
            "conv2d_240  Trainable = False\n",
            "conv2d_245  Trainable = False\n",
            "batch_normalization_240  Trainable = False\n",
            "batch_normalization_245  Trainable = False\n",
            "activation_240  Trainable = False\n",
            "activation_245  Trainable = False\n",
            "average_pooling2d_23  Trainable = False\n",
            "conv2d_238  Trainable = False\n",
            "conv2d_241  Trainable = False\n",
            "conv2d_246  Trainable = False\n",
            "conv2d_247  Trainable = False\n",
            "batch_normalization_238  Trainable = False\n",
            "batch_normalization_241  Trainable = False\n",
            "batch_normalization_246  Trainable = False\n",
            "batch_normalization_247  Trainable = False\n",
            "activation_238  Trainable = False\n",
            "activation_241  Trainable = False\n",
            "activation_246  Trainable = False\n",
            "activation_247  Trainable = False\n",
            "mixed6  Trainable = False\n",
            "conv2d_252  Trainable = False\n",
            "batch_normalization_252  Trainable = False\n",
            "activation_252  Trainable = False\n",
            "conv2d_253  Trainable = False\n",
            "batch_normalization_253  Trainable = False\n",
            "activation_253  Trainable = False\n",
            "conv2d_249  Trainable = False\n",
            "conv2d_254  Trainable = False\n",
            "batch_normalization_249  Trainable = False\n",
            "batch_normalization_254  Trainable = False\n",
            "activation_249  Trainable = False\n",
            "activation_254  Trainable = False\n",
            "conv2d_250  Trainable = False\n",
            "conv2d_255  Trainable = False\n",
            "batch_normalization_250  Trainable = False\n",
            "batch_normalization_255  Trainable = False\n",
            "activation_250  Trainable = False\n",
            "activation_255  Trainable = False\n",
            "average_pooling2d_24  Trainable = False\n",
            "conv2d_248  Trainable = False\n",
            "conv2d_251  Trainable = False\n",
            "conv2d_256  Trainable = False\n",
            "conv2d_257  Trainable = False\n",
            "batch_normalization_248  Trainable = False\n",
            "batch_normalization_251  Trainable = False\n",
            "batch_normalization_256  Trainable = False\n",
            "batch_normalization_257  Trainable = False\n",
            "activation_248  Trainable = False\n",
            "activation_251  Trainable = False\n",
            "activation_256  Trainable = False\n",
            "activation_257  Trainable = False\n",
            "mixed7  Trainable = False\n",
            "conv2d_260  Trainable = False\n",
            "batch_normalization_260  Trainable = False\n",
            "activation_260  Trainable = False\n",
            "conv2d_261  Trainable = False\n",
            "batch_normalization_261  Trainable = False\n",
            "activation_261  Trainable = False\n",
            "conv2d_258  Trainable = False\n",
            "conv2d_262  Trainable = False\n",
            "batch_normalization_258  Trainable = False\n",
            "batch_normalization_262  Trainable = False\n",
            "activation_258  Trainable = False\n",
            "activation_262  Trainable = False\n",
            "conv2d_259  Trainable = False\n",
            "conv2d_263  Trainable = False\n",
            "batch_normalization_259  Trainable = False\n",
            "batch_normalization_263  Trainable = False\n",
            "activation_259  Trainable = False\n",
            "activation_263  Trainable = False\n",
            "max_pooling2d_11  Trainable = False\n",
            "mixed8  Trainable = False\n",
            "conv2d_268  Trainable = False\n",
            "batch_normalization_268  Trainable = False\n",
            "activation_268  Trainable = False\n",
            "conv2d_265  Trainable = False\n",
            "conv2d_269  Trainable = False\n",
            "batch_normalization_265  Trainable = False\n",
            "batch_normalization_269  Trainable = False\n",
            "activation_265  Trainable = False\n",
            "activation_269  Trainable = False\n",
            "conv2d_266  Trainable = False\n",
            "conv2d_267  Trainable = False\n",
            "conv2d_270  Trainable = False\n",
            "conv2d_271  Trainable = False\n",
            "average_pooling2d_25  Trainable = False\n",
            "conv2d_264  Trainable = False\n",
            "batch_normalization_266  Trainable = False\n",
            "batch_normalization_267  Trainable = False\n",
            "batch_normalization_270  Trainable = False\n",
            "batch_normalization_271  Trainable = False\n",
            "conv2d_272  Trainable = False\n",
            "batch_normalization_264  Trainable = False\n",
            "activation_266  Trainable = False\n",
            "activation_267  Trainable = False\n",
            "activation_270  Trainable = False\n",
            "activation_271  Trainable = False\n",
            "batch_normalization_272  Trainable = False\n",
            "activation_264  Trainable = False\n",
            "mixed9_0  Trainable = False\n",
            "concatenate_4  Trainable = False\n",
            "activation_272  Trainable = False\n",
            "mixed9  Trainable = False\n",
            "conv2d_277  Trainable = False\n",
            "batch_normalization_277  Trainable = False\n",
            "activation_277  Trainable = False\n",
            "conv2d_274  Trainable = False\n",
            "conv2d_278  Trainable = False\n",
            "batch_normalization_274  Trainable = False\n",
            "batch_normalization_278  Trainable = False\n",
            "activation_274  Trainable = False\n",
            "activation_278  Trainable = False\n",
            "conv2d_275  Trainable = False\n",
            "conv2d_276  Trainable = False\n",
            "conv2d_279  Trainable = False\n",
            "conv2d_280  Trainable = False\n",
            "average_pooling2d_26  Trainable = False\n",
            "conv2d_273  Trainable = False\n",
            "batch_normalization_275  Trainable = False\n",
            "batch_normalization_276  Trainable = False\n",
            "batch_normalization_279  Trainable = False\n",
            "batch_normalization_280  Trainable = False\n",
            "conv2d_281  Trainable = False\n",
            "batch_normalization_273  Trainable = False\n",
            "activation_275  Trainable = False\n",
            "activation_276  Trainable = False\n",
            "activation_279  Trainable = False\n",
            "activation_280  Trainable = False\n",
            "batch_normalization_281  Trainable = False\n",
            "activation_273  Trainable = False\n",
            "mixed9_1  Trainable = False\n",
            "concatenate_5  Trainable = False\n",
            "activation_281  Trainable = False\n",
            "mixed10  Trainable = False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLAWoovdj2Zh",
        "colab_type": "code",
        "outputId": "da643b38-301c-4a94-f110-5e9581eef05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for layer in inception_model.layers:\n",
        "    print(layer.name,' Trainable =',layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_7  Trainable = False\n",
            "conv2d_188  Trainable = False\n",
            "batch_normalization_188  Trainable = False\n",
            "activation_188  Trainable = False\n",
            "conv2d_189  Trainable = False\n",
            "batch_normalization_189  Trainable = False\n",
            "activation_189  Trainable = False\n",
            "conv2d_190  Trainable = False\n",
            "batch_normalization_190  Trainable = False\n",
            "activation_190  Trainable = False\n",
            "max_pooling2d_8  Trainable = False\n",
            "conv2d_191  Trainable = False\n",
            "batch_normalization_191  Trainable = False\n",
            "activation_191  Trainable = False\n",
            "conv2d_192  Trainable = False\n",
            "batch_normalization_192  Trainable = False\n",
            "activation_192  Trainable = False\n",
            "max_pooling2d_9  Trainable = False\n",
            "conv2d_196  Trainable = False\n",
            "batch_normalization_196  Trainable = False\n",
            "activation_196  Trainable = False\n",
            "conv2d_194  Trainable = False\n",
            "conv2d_197  Trainable = False\n",
            "batch_normalization_194  Trainable = False\n",
            "batch_normalization_197  Trainable = False\n",
            "activation_194  Trainable = False\n",
            "activation_197  Trainable = False\n",
            "average_pooling2d_18  Trainable = False\n",
            "conv2d_193  Trainable = False\n",
            "conv2d_195  Trainable = False\n",
            "conv2d_198  Trainable = False\n",
            "conv2d_199  Trainable = False\n",
            "batch_normalization_193  Trainable = False\n",
            "batch_normalization_195  Trainable = False\n",
            "batch_normalization_198  Trainable = False\n",
            "batch_normalization_199  Trainable = False\n",
            "activation_193  Trainable = False\n",
            "activation_195  Trainable = False\n",
            "activation_198  Trainable = False\n",
            "activation_199  Trainable = False\n",
            "mixed0  Trainable = False\n",
            "conv2d_203  Trainable = False\n",
            "batch_normalization_203  Trainable = False\n",
            "activation_203  Trainable = False\n",
            "conv2d_201  Trainable = False\n",
            "conv2d_204  Trainable = False\n",
            "batch_normalization_201  Trainable = False\n",
            "batch_normalization_204  Trainable = False\n",
            "activation_201  Trainable = False\n",
            "activation_204  Trainable = False\n",
            "average_pooling2d_19  Trainable = False\n",
            "conv2d_200  Trainable = False\n",
            "conv2d_202  Trainable = False\n",
            "conv2d_205  Trainable = False\n",
            "conv2d_206  Trainable = False\n",
            "batch_normalization_200  Trainable = False\n",
            "batch_normalization_202  Trainable = False\n",
            "batch_normalization_205  Trainable = False\n",
            "batch_normalization_206  Trainable = False\n",
            "activation_200  Trainable = False\n",
            "activation_202  Trainable = False\n",
            "activation_205  Trainable = False\n",
            "activation_206  Trainable = False\n",
            "mixed1  Trainable = False\n",
            "conv2d_210  Trainable = False\n",
            "batch_normalization_210  Trainable = False\n",
            "activation_210  Trainable = False\n",
            "conv2d_208  Trainable = False\n",
            "conv2d_211  Trainable = False\n",
            "batch_normalization_208  Trainable = False\n",
            "batch_normalization_211  Trainable = False\n",
            "activation_208  Trainable = False\n",
            "activation_211  Trainable = False\n",
            "average_pooling2d_20  Trainable = False\n",
            "conv2d_207  Trainable = False\n",
            "conv2d_209  Trainable = False\n",
            "conv2d_212  Trainable = False\n",
            "conv2d_213  Trainable = False\n",
            "batch_normalization_207  Trainable = False\n",
            "batch_normalization_209  Trainable = False\n",
            "batch_normalization_212  Trainable = False\n",
            "batch_normalization_213  Trainable = False\n",
            "activation_207  Trainable = False\n",
            "activation_209  Trainable = False\n",
            "activation_212  Trainable = False\n",
            "activation_213  Trainable = False\n",
            "mixed2  Trainable = False\n",
            "conv2d_215  Trainable = False\n",
            "batch_normalization_215  Trainable = False\n",
            "activation_215  Trainable = False\n",
            "conv2d_216  Trainable = False\n",
            "batch_normalization_216  Trainable = False\n",
            "activation_216  Trainable = False\n",
            "conv2d_214  Trainable = False\n",
            "conv2d_217  Trainable = False\n",
            "batch_normalization_214  Trainable = False\n",
            "batch_normalization_217  Trainable = False\n",
            "activation_214  Trainable = False\n",
            "activation_217  Trainable = False\n",
            "max_pooling2d_10  Trainable = False\n",
            "mixed3  Trainable = False\n",
            "conv2d_222  Trainable = False\n",
            "batch_normalization_222  Trainable = False\n",
            "activation_222  Trainable = False\n",
            "conv2d_223  Trainable = False\n",
            "batch_normalization_223  Trainable = False\n",
            "activation_223  Trainable = False\n",
            "conv2d_219  Trainable = False\n",
            "conv2d_224  Trainable = False\n",
            "batch_normalization_219  Trainable = False\n",
            "batch_normalization_224  Trainable = False\n",
            "activation_219  Trainable = False\n",
            "activation_224  Trainable = False\n",
            "conv2d_220  Trainable = False\n",
            "conv2d_225  Trainable = False\n",
            "batch_normalization_220  Trainable = False\n",
            "batch_normalization_225  Trainable = False\n",
            "activation_220  Trainable = False\n",
            "activation_225  Trainable = False\n",
            "average_pooling2d_21  Trainable = False\n",
            "conv2d_218  Trainable = False\n",
            "conv2d_221  Trainable = False\n",
            "conv2d_226  Trainable = False\n",
            "conv2d_227  Trainable = False\n",
            "batch_normalization_218  Trainable = False\n",
            "batch_normalization_221  Trainable = False\n",
            "batch_normalization_226  Trainable = False\n",
            "batch_normalization_227  Trainable = False\n",
            "activation_218  Trainable = False\n",
            "activation_221  Trainable = False\n",
            "activation_226  Trainable = False\n",
            "activation_227  Trainable = False\n",
            "mixed4  Trainable = False\n",
            "conv2d_232  Trainable = False\n",
            "batch_normalization_232  Trainable = False\n",
            "activation_232  Trainable = False\n",
            "conv2d_233  Trainable = False\n",
            "batch_normalization_233  Trainable = False\n",
            "activation_233  Trainable = False\n",
            "conv2d_229  Trainable = False\n",
            "conv2d_234  Trainable = False\n",
            "batch_normalization_229  Trainable = False\n",
            "batch_normalization_234  Trainable = False\n",
            "activation_229  Trainable = False\n",
            "activation_234  Trainable = False\n",
            "conv2d_230  Trainable = False\n",
            "conv2d_235  Trainable = False\n",
            "batch_normalization_230  Trainable = False\n",
            "batch_normalization_235  Trainable = False\n",
            "activation_230  Trainable = False\n",
            "activation_235  Trainable = False\n",
            "average_pooling2d_22  Trainable = False\n",
            "conv2d_228  Trainable = False\n",
            "conv2d_231  Trainable = False\n",
            "conv2d_236  Trainable = False\n",
            "conv2d_237  Trainable = False\n",
            "batch_normalization_228  Trainable = False\n",
            "batch_normalization_231  Trainable = False\n",
            "batch_normalization_236  Trainable = False\n",
            "batch_normalization_237  Trainable = False\n",
            "activation_228  Trainable = False\n",
            "activation_231  Trainable = False\n",
            "activation_236  Trainable = False\n",
            "activation_237  Trainable = False\n",
            "mixed5  Trainable = False\n",
            "conv2d_242  Trainable = False\n",
            "batch_normalization_242  Trainable = False\n",
            "activation_242  Trainable = False\n",
            "conv2d_243  Trainable = False\n",
            "batch_normalization_243  Trainable = False\n",
            "activation_243  Trainable = False\n",
            "conv2d_239  Trainable = False\n",
            "conv2d_244  Trainable = False\n",
            "batch_normalization_239  Trainable = False\n",
            "batch_normalization_244  Trainable = False\n",
            "activation_239  Trainable = False\n",
            "activation_244  Trainable = False\n",
            "conv2d_240  Trainable = False\n",
            "conv2d_245  Trainable = False\n",
            "batch_normalization_240  Trainable = False\n",
            "batch_normalization_245  Trainable = False\n",
            "activation_240  Trainable = False\n",
            "activation_245  Trainable = False\n",
            "average_pooling2d_23  Trainable = False\n",
            "conv2d_238  Trainable = False\n",
            "conv2d_241  Trainable = False\n",
            "conv2d_246  Trainable = False\n",
            "conv2d_247  Trainable = False\n",
            "batch_normalization_238  Trainable = False\n",
            "batch_normalization_241  Trainable = False\n",
            "batch_normalization_246  Trainable = False\n",
            "batch_normalization_247  Trainable = False\n",
            "activation_238  Trainable = False\n",
            "activation_241  Trainable = False\n",
            "activation_246  Trainable = False\n",
            "activation_247  Trainable = False\n",
            "mixed6  Trainable = False\n",
            "conv2d_252  Trainable = False\n",
            "batch_normalization_252  Trainable = False\n",
            "activation_252  Trainable = False\n",
            "conv2d_253  Trainable = False\n",
            "batch_normalization_253  Trainable = False\n",
            "activation_253  Trainable = False\n",
            "conv2d_249  Trainable = False\n",
            "conv2d_254  Trainable = False\n",
            "batch_normalization_249  Trainable = False\n",
            "batch_normalization_254  Trainable = False\n",
            "activation_249  Trainable = False\n",
            "activation_254  Trainable = False\n",
            "conv2d_250  Trainable = False\n",
            "conv2d_255  Trainable = False\n",
            "batch_normalization_250  Trainable = False\n",
            "batch_normalization_255  Trainable = False\n",
            "activation_250  Trainable = False\n",
            "activation_255  Trainable = False\n",
            "average_pooling2d_24  Trainable = False\n",
            "conv2d_248  Trainable = False\n",
            "conv2d_251  Trainable = False\n",
            "conv2d_256  Trainable = False\n",
            "conv2d_257  Trainable = False\n",
            "batch_normalization_248  Trainable = False\n",
            "batch_normalization_251  Trainable = False\n",
            "batch_normalization_256  Trainable = False\n",
            "batch_normalization_257  Trainable = False\n",
            "activation_248  Trainable = False\n",
            "activation_251  Trainable = False\n",
            "activation_256  Trainable = False\n",
            "activation_257  Trainable = False\n",
            "mixed7  Trainable = False\n",
            "conv2d_260  Trainable = False\n",
            "batch_normalization_260  Trainable = False\n",
            "activation_260  Trainable = False\n",
            "conv2d_261  Trainable = False\n",
            "batch_normalization_261  Trainable = False\n",
            "activation_261  Trainable = False\n",
            "conv2d_258  Trainable = False\n",
            "conv2d_262  Trainable = False\n",
            "batch_normalization_258  Trainable = False\n",
            "batch_normalization_262  Trainable = False\n",
            "activation_258  Trainable = False\n",
            "activation_262  Trainable = False\n",
            "conv2d_259  Trainable = False\n",
            "conv2d_263  Trainable = False\n",
            "batch_normalization_259  Trainable = False\n",
            "batch_normalization_263  Trainable = False\n",
            "activation_259  Trainable = False\n",
            "activation_263  Trainable = False\n",
            "max_pooling2d_11  Trainable = False\n",
            "mixed8  Trainable = False\n",
            "conv2d_268  Trainable = False\n",
            "batch_normalization_268  Trainable = False\n",
            "activation_268  Trainable = False\n",
            "conv2d_265  Trainable = False\n",
            "conv2d_269  Trainable = False\n",
            "batch_normalization_265  Trainable = False\n",
            "batch_normalization_269  Trainable = False\n",
            "activation_265  Trainable = False\n",
            "activation_269  Trainable = False\n",
            "conv2d_266  Trainable = False\n",
            "conv2d_267  Trainable = False\n",
            "conv2d_270  Trainable = False\n",
            "conv2d_271  Trainable = False\n",
            "average_pooling2d_25  Trainable = False\n",
            "conv2d_264  Trainable = False\n",
            "batch_normalization_266  Trainable = False\n",
            "batch_normalization_267  Trainable = False\n",
            "batch_normalization_270  Trainable = False\n",
            "batch_normalization_271  Trainable = False\n",
            "conv2d_272  Trainable = False\n",
            "batch_normalization_264  Trainable = False\n",
            "activation_266  Trainable = False\n",
            "activation_267  Trainable = False\n",
            "activation_270  Trainable = False\n",
            "activation_271  Trainable = False\n",
            "batch_normalization_272  Trainable = False\n",
            "activation_264  Trainable = False\n",
            "mixed9_0  Trainable = False\n",
            "concatenate_4  Trainable = False\n",
            "activation_272  Trainable = False\n",
            "mixed9  Trainable = False\n",
            "conv2d_277  Trainable = False\n",
            "batch_normalization_277  Trainable = False\n",
            "activation_277  Trainable = False\n",
            "conv2d_274  Trainable = False\n",
            "conv2d_278  Trainable = False\n",
            "batch_normalization_274  Trainable = False\n",
            "batch_normalization_278  Trainable = False\n",
            "activation_274  Trainable = False\n",
            "activation_278  Trainable = False\n",
            "conv2d_275  Trainable = False\n",
            "conv2d_276  Trainable = False\n",
            "conv2d_279  Trainable = False\n",
            "conv2d_280  Trainable = False\n",
            "average_pooling2d_26  Trainable = False\n",
            "conv2d_273  Trainable = False\n",
            "batch_normalization_275  Trainable = False\n",
            "batch_normalization_276  Trainable = False\n",
            "batch_normalization_279  Trainable = False\n",
            "batch_normalization_280  Trainable = False\n",
            "conv2d_281  Trainable = False\n",
            "batch_normalization_273  Trainable = False\n",
            "activation_275  Trainable = False\n",
            "activation_276  Trainable = False\n",
            "activation_279  Trainable = False\n",
            "activation_280  Trainable = False\n",
            "batch_normalization_281  Trainable = False\n",
            "activation_273  Trainable = False\n",
            "mixed9_1  Trainable = False\n",
            "concatenate_5  Trainable = False\n",
            "activation_281  Trainable = False\n",
            "mixed10  Trainable = False\n",
            "global_average_pooling2d_4  Trainable = True\n",
            "dense_10  Trainable = True\n",
            "dropout_7  Trainable = True\n",
            "dense_11  Trainable = True\n",
            "dropout_8  Trainable = True\n",
            "dense_12  Trainable = True\n",
            "dropout_9  Trainable = True\n",
            "dense_13  Trainable = True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLbZOgbPkay5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#opt = tf.keras.optimizers.Adam(lr=0.001)\n",
        "opt = tf.keras.optimizers.RMSprop(lr=1e-4)\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "inception_model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5QTzooTlgct",
        "colab_type": "code",
        "outputId": "fe15620c-f685-4906-9016-6235bbcbe6b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train_steps = int(len(df)/batch_size) #total trains set / batch_size\n",
        "val_steps = int(len(df_val)/batch_size)\n",
        "\n",
        "\n",
        "print('train steps:',train_steps)\n",
        "print('val steps:',val_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train steps: 46\n",
            "val steps: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpEN4e-8lRki",
        "colab_type": "code",
        "outputId": "4125622e-7c81-47a4-aa17-c513c2c4eece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "### Train the model with validation \n",
        "inception_history = inception_model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 46 steps, validate for 7 steps\n",
            "Epoch 1/10\n",
            "46/46 [==============================] - 112s 2s/step - loss: 1.0681 - accuracy: 0.4331 - val_loss: 9.3817 - val_accuracy: 0.3571\n",
            "Epoch 2/10\n",
            "46/46 [==============================] - 21s 453ms/step - loss: 0.8809 - accuracy: 0.5688 - val_loss: 12.7614 - val_accuracy: 0.3203\n",
            "Epoch 3/10\n",
            "46/46 [==============================] - 32s 688ms/step - loss: 0.7727 - accuracy: 0.6174 - val_loss: 22.0767 - val_accuracy: 0.3047\n",
            "Epoch 4/10\n",
            "46/46 [==============================] - 21s 456ms/step - loss: 0.7191 - accuracy: 0.6512 - val_loss: 26.6182 - val_accuracy: 0.3058\n",
            "Epoch 5/10\n",
            "46/46 [==============================] - 30s 661ms/step - loss: 0.6816 - accuracy: 0.6641 - val_loss: 31.1755 - val_accuracy: 0.1708\n",
            "Epoch 6/10\n",
            "46/46 [==============================] - 21s 450ms/step - loss: 0.6614 - accuracy: 0.6870 - val_loss: 39.5757 - val_accuracy: 0.1953\n",
            "Epoch 7/10\n",
            "46/46 [==============================] - 21s 453ms/step - loss: 0.6513 - accuracy: 0.6945 - val_loss: 32.9595 - val_accuracy: 0.1562\n",
            "Epoch 8/10\n",
            "46/46 [==============================] - 30s 654ms/step - loss: 0.6327 - accuracy: 0.7033 - val_loss: 42.7075 - val_accuracy: 0.1228\n",
            "Epoch 9/10\n",
            "46/46 [==============================] - 21s 451ms/step - loss: 0.6087 - accuracy: 0.7118 - val_loss: 45.0261 - val_accuracy: 0.1172\n",
            "Epoch 10/10\n",
            "46/46 [==============================] - 30s 660ms/step - loss: 0.6023 - accuracy: 0.7223 - val_loss: 34.9493 - val_accuracy: 0.1853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_wj1elhlT5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inception_model.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in inception_model.layers:\n",
        "    if layer.name == 'conv2d_181':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4473X2c4mxE",
        "colab_type": "code",
        "outputId": "05b25dc0-c2d9-4563-d1b4-151611060029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for layer in inception_model.layers:\n",
        "    print(layer.name,' Trainable =',layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_5  Trainable = False\n",
            "conv2d  Trainable = False\n",
            "batch_normalization  Trainable = False\n",
            "activation  Trainable = False\n",
            "conv2d_1  Trainable = False\n",
            "batch_normalization_1  Trainable = False\n",
            "activation_1  Trainable = False\n",
            "conv2d_2  Trainable = False\n",
            "batch_normalization_2  Trainable = False\n",
            "activation_2  Trainable = False\n",
            "max_pooling2d  Trainable = False\n",
            "conv2d_3  Trainable = False\n",
            "batch_normalization_3  Trainable = False\n",
            "activation_3  Trainable = False\n",
            "conv2d_4  Trainable = False\n",
            "batch_normalization_4  Trainable = False\n",
            "activation_4  Trainable = False\n",
            "max_pooling2d_1  Trainable = False\n",
            "conv2d_8  Trainable = False\n",
            "batch_normalization_8  Trainable = False\n",
            "activation_8  Trainable = False\n",
            "conv2d_6  Trainable = False\n",
            "conv2d_9  Trainable = False\n",
            "batch_normalization_6  Trainable = False\n",
            "batch_normalization_9  Trainable = False\n",
            "activation_6  Trainable = False\n",
            "activation_9  Trainable = False\n",
            "average_pooling2d  Trainable = False\n",
            "conv2d_5  Trainable = False\n",
            "conv2d_7  Trainable = False\n",
            "conv2d_10  Trainable = False\n",
            "conv2d_11  Trainable = False\n",
            "batch_normalization_5  Trainable = False\n",
            "batch_normalization_7  Trainable = False\n",
            "batch_normalization_10  Trainable = False\n",
            "batch_normalization_11  Trainable = False\n",
            "activation_5  Trainable = False\n",
            "activation_7  Trainable = False\n",
            "activation_10  Trainable = False\n",
            "activation_11  Trainable = False\n",
            "mixed0  Trainable = False\n",
            "conv2d_15  Trainable = False\n",
            "batch_normalization_15  Trainable = False\n",
            "activation_15  Trainable = False\n",
            "conv2d_13  Trainable = False\n",
            "conv2d_16  Trainable = False\n",
            "batch_normalization_13  Trainable = False\n",
            "batch_normalization_16  Trainable = False\n",
            "activation_13  Trainable = False\n",
            "activation_16  Trainable = False\n",
            "average_pooling2d_1  Trainable = False\n",
            "conv2d_12  Trainable = False\n",
            "conv2d_14  Trainable = False\n",
            "conv2d_17  Trainable = False\n",
            "conv2d_18  Trainable = False\n",
            "batch_normalization_12  Trainable = False\n",
            "batch_normalization_14  Trainable = False\n",
            "batch_normalization_17  Trainable = False\n",
            "batch_normalization_18  Trainable = False\n",
            "activation_12  Trainable = False\n",
            "activation_14  Trainable = False\n",
            "activation_17  Trainable = False\n",
            "activation_18  Trainable = False\n",
            "mixed1  Trainable = False\n",
            "conv2d_22  Trainable = False\n",
            "batch_normalization_22  Trainable = False\n",
            "activation_22  Trainable = False\n",
            "conv2d_20  Trainable = False\n",
            "conv2d_23  Trainable = False\n",
            "batch_normalization_20  Trainable = False\n",
            "batch_normalization_23  Trainable = False\n",
            "activation_20  Trainable = False\n",
            "activation_23  Trainable = False\n",
            "average_pooling2d_2  Trainable = False\n",
            "conv2d_19  Trainable = False\n",
            "conv2d_21  Trainable = False\n",
            "conv2d_24  Trainable = False\n",
            "conv2d_25  Trainable = False\n",
            "batch_normalization_19  Trainable = False\n",
            "batch_normalization_21  Trainable = False\n",
            "batch_normalization_24  Trainable = False\n",
            "batch_normalization_25  Trainable = False\n",
            "activation_19  Trainable = False\n",
            "activation_21  Trainable = False\n",
            "activation_24  Trainable = False\n",
            "activation_25  Trainable = False\n",
            "mixed2  Trainable = False\n",
            "conv2d_27  Trainable = False\n",
            "batch_normalization_27  Trainable = False\n",
            "activation_27  Trainable = False\n",
            "conv2d_28  Trainable = False\n",
            "batch_normalization_28  Trainable = False\n",
            "activation_28  Trainable = False\n",
            "conv2d_26  Trainable = False\n",
            "conv2d_29  Trainable = False\n",
            "batch_normalization_26  Trainable = False\n",
            "batch_normalization_29  Trainable = False\n",
            "activation_26  Trainable = False\n",
            "activation_29  Trainable = False\n",
            "max_pooling2d_2  Trainable = False\n",
            "mixed3  Trainable = False\n",
            "conv2d_34  Trainable = False\n",
            "batch_normalization_34  Trainable = False\n",
            "activation_34  Trainable = False\n",
            "conv2d_35  Trainable = False\n",
            "batch_normalization_35  Trainable = False\n",
            "activation_35  Trainable = False\n",
            "conv2d_31  Trainable = False\n",
            "conv2d_36  Trainable = False\n",
            "batch_normalization_31  Trainable = False\n",
            "batch_normalization_36  Trainable = False\n",
            "activation_31  Trainable = False\n",
            "activation_36  Trainable = False\n",
            "conv2d_32  Trainable = False\n",
            "conv2d_37  Trainable = False\n",
            "batch_normalization_32  Trainable = False\n",
            "batch_normalization_37  Trainable = False\n",
            "activation_32  Trainable = False\n",
            "activation_37  Trainable = False\n",
            "average_pooling2d_3  Trainable = False\n",
            "conv2d_30  Trainable = False\n",
            "conv2d_33  Trainable = False\n",
            "conv2d_38  Trainable = False\n",
            "conv2d_39  Trainable = False\n",
            "batch_normalization_30  Trainable = False\n",
            "batch_normalization_33  Trainable = False\n",
            "batch_normalization_38  Trainable = False\n",
            "batch_normalization_39  Trainable = False\n",
            "activation_30  Trainable = False\n",
            "activation_33  Trainable = False\n",
            "activation_38  Trainable = False\n",
            "activation_39  Trainable = False\n",
            "mixed4  Trainable = False\n",
            "conv2d_44  Trainable = False\n",
            "batch_normalization_44  Trainable = False\n",
            "activation_44  Trainable = False\n",
            "conv2d_45  Trainable = False\n",
            "batch_normalization_45  Trainable = False\n",
            "activation_45  Trainable = False\n",
            "conv2d_41  Trainable = False\n",
            "conv2d_46  Trainable = False\n",
            "batch_normalization_41  Trainable = False\n",
            "batch_normalization_46  Trainable = False\n",
            "activation_41  Trainable = False\n",
            "activation_46  Trainable = False\n",
            "conv2d_42  Trainable = False\n",
            "conv2d_47  Trainable = False\n",
            "batch_normalization_42  Trainable = False\n",
            "batch_normalization_47  Trainable = False\n",
            "activation_42  Trainable = False\n",
            "activation_47  Trainable = False\n",
            "average_pooling2d_4  Trainable = False\n",
            "conv2d_40  Trainable = False\n",
            "conv2d_43  Trainable = False\n",
            "conv2d_48  Trainable = False\n",
            "conv2d_49  Trainable = False\n",
            "batch_normalization_40  Trainable = False\n",
            "batch_normalization_43  Trainable = False\n",
            "batch_normalization_48  Trainable = False\n",
            "batch_normalization_49  Trainable = False\n",
            "activation_40  Trainable = False\n",
            "activation_43  Trainable = False\n",
            "activation_48  Trainable = False\n",
            "activation_49  Trainable = False\n",
            "mixed5  Trainable = False\n",
            "conv2d_54  Trainable = False\n",
            "batch_normalization_54  Trainable = False\n",
            "activation_54  Trainable = False\n",
            "conv2d_55  Trainable = False\n",
            "batch_normalization_55  Trainable = False\n",
            "activation_55  Trainable = False\n",
            "conv2d_51  Trainable = False\n",
            "conv2d_56  Trainable = False\n",
            "batch_normalization_51  Trainable = False\n",
            "batch_normalization_56  Trainable = False\n",
            "activation_51  Trainable = False\n",
            "activation_56  Trainable = False\n",
            "conv2d_52  Trainable = False\n",
            "conv2d_57  Trainable = False\n",
            "batch_normalization_52  Trainable = False\n",
            "batch_normalization_57  Trainable = False\n",
            "activation_52  Trainable = False\n",
            "activation_57  Trainable = False\n",
            "average_pooling2d_5  Trainable = False\n",
            "conv2d_50  Trainable = False\n",
            "conv2d_53  Trainable = False\n",
            "conv2d_58  Trainable = False\n",
            "conv2d_59  Trainable = False\n",
            "batch_normalization_50  Trainable = False\n",
            "batch_normalization_53  Trainable = False\n",
            "batch_normalization_58  Trainable = False\n",
            "batch_normalization_59  Trainable = False\n",
            "activation_50  Trainable = False\n",
            "activation_53  Trainable = False\n",
            "activation_58  Trainable = False\n",
            "activation_59  Trainable = False\n",
            "mixed6  Trainable = False\n",
            "conv2d_64  Trainable = False\n",
            "batch_normalization_64  Trainable = False\n",
            "activation_64  Trainable = False\n",
            "conv2d_65  Trainable = False\n",
            "batch_normalization_65  Trainable = False\n",
            "activation_65  Trainable = False\n",
            "conv2d_61  Trainable = False\n",
            "conv2d_66  Trainable = False\n",
            "batch_normalization_61  Trainable = False\n",
            "batch_normalization_66  Trainable = False\n",
            "activation_61  Trainable = False\n",
            "activation_66  Trainable = False\n",
            "conv2d_62  Trainable = False\n",
            "conv2d_67  Trainable = False\n",
            "batch_normalization_62  Trainable = False\n",
            "batch_normalization_67  Trainable = False\n",
            "activation_62  Trainable = False\n",
            "activation_67  Trainable = False\n",
            "average_pooling2d_6  Trainable = False\n",
            "conv2d_60  Trainable = False\n",
            "conv2d_63  Trainable = False\n",
            "conv2d_68  Trainable = False\n",
            "conv2d_69  Trainable = False\n",
            "batch_normalization_60  Trainable = False\n",
            "batch_normalization_63  Trainable = False\n",
            "batch_normalization_68  Trainable = False\n",
            "batch_normalization_69  Trainable = False\n",
            "activation_60  Trainable = False\n",
            "activation_63  Trainable = False\n",
            "activation_68  Trainable = False\n",
            "activation_69  Trainable = False\n",
            "mixed7  Trainable = False\n",
            "conv2d_72  Trainable = False\n",
            "batch_normalization_72  Trainable = False\n",
            "activation_72  Trainable = False\n",
            "conv2d_73  Trainable = False\n",
            "batch_normalization_73  Trainable = False\n",
            "activation_73  Trainable = False\n",
            "conv2d_70  Trainable = False\n",
            "conv2d_74  Trainable = False\n",
            "batch_normalization_70  Trainable = False\n",
            "batch_normalization_74  Trainable = False\n",
            "activation_70  Trainable = False\n",
            "activation_74  Trainable = False\n",
            "conv2d_71  Trainable = False\n",
            "conv2d_75  Trainable = False\n",
            "batch_normalization_71  Trainable = False\n",
            "batch_normalization_75  Trainable = False\n",
            "activation_71  Trainable = False\n",
            "activation_75  Trainable = False\n",
            "max_pooling2d_3  Trainable = False\n",
            "mixed8  Trainable = False\n",
            "conv2d_80  Trainable = False\n",
            "batch_normalization_80  Trainable = False\n",
            "activation_80  Trainable = False\n",
            "conv2d_77  Trainable = False\n",
            "conv2d_81  Trainable = False\n",
            "batch_normalization_77  Trainable = False\n",
            "batch_normalization_81  Trainable = False\n",
            "activation_77  Trainable = False\n",
            "activation_81  Trainable = False\n",
            "conv2d_78  Trainable = False\n",
            "conv2d_79  Trainable = False\n",
            "conv2d_82  Trainable = False\n",
            "conv2d_83  Trainable = False\n",
            "average_pooling2d_7  Trainable = False\n",
            "conv2d_76  Trainable = False\n",
            "batch_normalization_78  Trainable = False\n",
            "batch_normalization_79  Trainable = False\n",
            "batch_normalization_82  Trainable = False\n",
            "batch_normalization_83  Trainable = False\n",
            "conv2d_84  Trainable = False\n",
            "batch_normalization_76  Trainable = False\n",
            "activation_78  Trainable = False\n",
            "activation_79  Trainable = False\n",
            "activation_82  Trainable = False\n",
            "activation_83  Trainable = False\n",
            "batch_normalization_84  Trainable = False\n",
            "activation_76  Trainable = False\n",
            "mixed9_0  Trainable = False\n",
            "concatenate  Trainable = False\n",
            "activation_84  Trainable = False\n",
            "mixed9  Trainable = False\n",
            "conv2d_89  Trainable = False\n",
            "batch_normalization_89  Trainable = False\n",
            "activation_89  Trainable = False\n",
            "conv2d_86  Trainable = False\n",
            "conv2d_90  Trainable = False\n",
            "batch_normalization_86  Trainable = False\n",
            "batch_normalization_90  Trainable = False\n",
            "activation_86  Trainable = False\n",
            "activation_90  Trainable = False\n",
            "conv2d_87  Trainable = False\n",
            "conv2d_88  Trainable = False\n",
            "conv2d_91  Trainable = False\n",
            "conv2d_92  Trainable = False\n",
            "average_pooling2d_8  Trainable = False\n",
            "conv2d_85  Trainable = False\n",
            "batch_normalization_87  Trainable = False\n",
            "batch_normalization_88  Trainable = False\n",
            "batch_normalization_91  Trainable = False\n",
            "batch_normalization_92  Trainable = False\n",
            "conv2d_93  Trainable = False\n",
            "batch_normalization_85  Trainable = False\n",
            "activation_87  Trainable = False\n",
            "activation_88  Trainable = False\n",
            "activation_91  Trainable = False\n",
            "activation_92  Trainable = False\n",
            "batch_normalization_93  Trainable = False\n",
            "activation_85  Trainable = False\n",
            "mixed9_1  Trainable = False\n",
            "concatenate_1  Trainable = False\n",
            "activation_93  Trainable = False\n",
            "mixed10  Trainable = False\n",
            "global_average_pooling2d_3  Trainable = True\n",
            "dense_6  Trainable = True\n",
            "dropout_4  Trainable = True\n",
            "dense_7  Trainable = True\n",
            "dropout_5  Trainable = True\n",
            "dense_8  Trainable = True\n",
            "dropout_6  Trainable = True\n",
            "dense_9  Trainable = True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwivZWYZ4p2p",
        "colab_type": "code",
        "outputId": "6b39b33b-ca5e-46fc-b6ec-825b8a8889ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 50\n",
        "\n",
        "\t\n",
        "es = EarlyStopping(monitor='accuracy', mode='max', verbose=1, patience=50)\n",
        "\n",
        "# Train the model with validation \n",
        "tuned_inception_history = inception_model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps,\n",
        "                   callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 17 steps, validate for 2 steps\n",
            "Epoch 1/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.3618 - accuracy: 0.8463 - val_loss: 37.8424 - val_accuracy: 0.3184\n",
            "Epoch 2/50\n",
            "17/17 [==============================] - 12s 722ms/step - loss: 0.3589 - accuracy: 0.8421 - val_loss: 49.6048 - val_accuracy: 0.3320\n",
            "Epoch 3/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.3792 - accuracy: 0.8304 - val_loss: 59.1348 - val_accuracy: 0.3457\n",
            "Epoch 4/50\n",
            "17/17 [==============================] - 12s 725ms/step - loss: 0.3308 - accuracy: 0.8591 - val_loss: 57.4081 - val_accuracy: 0.3301\n",
            "Epoch 5/50\n",
            "17/17 [==============================] - 12s 716ms/step - loss: 0.3058 - accuracy: 0.8697 - val_loss: 110.4535 - val_accuracy: 0.3516\n",
            "Epoch 6/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2873 - accuracy: 0.8780 - val_loss: 84.0913 - val_accuracy: 0.3477\n",
            "Epoch 7/50\n",
            "17/17 [==============================] - 12s 715ms/step - loss: 0.2744 - accuracy: 0.8860 - val_loss: 63.6652 - val_accuracy: 0.3516\n",
            "Epoch 8/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2700 - accuracy: 0.8897 - val_loss: 119.9577 - val_accuracy: 0.3594\n",
            "Epoch 9/50\n",
            "17/17 [==============================] - 12s 717ms/step - loss: 0.2619 - accuracy: 0.8941 - val_loss: 156.8447 - val_accuracy: 0.3379\n",
            "Epoch 10/50\n",
            "17/17 [==============================] - 12s 723ms/step - loss: 0.2629 - accuracy: 0.8927 - val_loss: 68.9920 - val_accuracy: 0.3027\n",
            "Epoch 11/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2470 - accuracy: 0.9017 - val_loss: 78.1851 - val_accuracy: 0.3262\n",
            "Epoch 12/50\n",
            "17/17 [==============================] - 12s 721ms/step - loss: 0.2214 - accuracy: 0.9072 - val_loss: 51.5826 - val_accuracy: 0.3066\n",
            "Epoch 13/50\n",
            "17/17 [==============================] - 13s 756ms/step - loss: 0.2134 - accuracy: 0.9187 - val_loss: 63.6360 - val_accuracy: 0.3262\n",
            "Epoch 14/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1923 - accuracy: 0.9246 - val_loss: 106.9960 - val_accuracy: 0.3594\n",
            "Epoch 15/50\n",
            "17/17 [==============================] - 12s 719ms/step - loss: 0.2071 - accuracy: 0.9173 - val_loss: 50.3701 - val_accuracy: 0.3535\n",
            "Epoch 16/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1951 - accuracy: 0.9203 - val_loss: 72.4279 - val_accuracy: 0.3086\n",
            "Epoch 17/50\n",
            "17/17 [==============================] - 12s 710ms/step - loss: 0.1853 - accuracy: 0.9246 - val_loss: 63.4357 - val_accuracy: 0.3730\n",
            "Epoch 18/50\n",
            "17/17 [==============================] - 12s 711ms/step - loss: 0.1874 - accuracy: 0.9251 - val_loss: 91.2361 - val_accuracy: 0.3203\n",
            "Epoch 19/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1812 - accuracy: 0.9299 - val_loss: 91.0556 - val_accuracy: 0.3359\n",
            "Epoch 20/50\n",
            "17/17 [==============================] - 12s 708ms/step - loss: 0.1756 - accuracy: 0.9320 - val_loss: 115.1484 - val_accuracy: 0.3105\n",
            "Epoch 21/50\n",
            "17/17 [==============================] - 19s 1s/step - loss: 0.2218 - accuracy: 0.9108 - val_loss: 68.5948 - val_accuracy: 0.2852\n",
            "Epoch 22/50\n",
            "17/17 [==============================] - 12s 713ms/step - loss: 0.2095 - accuracy: 0.9092 - val_loss: 80.7701 - val_accuracy: 0.3477\n",
            "Epoch 23/50\n",
            "17/17 [==============================] - 12s 710ms/step - loss: 0.2308 - accuracy: 0.9120 - val_loss: 117.6952 - val_accuracy: 0.3340\n",
            "Epoch 24/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2244 - accuracy: 0.9088 - val_loss: 107.7353 - val_accuracy: 0.3301\n",
            "Epoch 25/50\n",
            "17/17 [==============================] - 12s 708ms/step - loss: 0.2291 - accuracy: 0.9074 - val_loss: 73.4982 - val_accuracy: 0.3008\n",
            "Epoch 26/50\n",
            "17/17 [==============================] - 12s 732ms/step - loss: 0.2374 - accuracy: 0.9060 - val_loss: 87.0000 - val_accuracy: 0.3301\n",
            "Epoch 27/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2045 - accuracy: 0.9198 - val_loss: 109.5618 - val_accuracy: 0.3379\n",
            "Epoch 28/50\n",
            "17/17 [==============================] - 12s 718ms/step - loss: 0.2017 - accuracy: 0.9191 - val_loss: 70.5089 - val_accuracy: 0.3496\n",
            "Epoch 29/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1889 - accuracy: 0.9244 - val_loss: 59.7646 - val_accuracy: 0.3477\n",
            "Epoch 30/50\n",
            "17/17 [==============================] - 12s 707ms/step - loss: 0.1932 - accuracy: 0.9233 - val_loss: 80.2912 - val_accuracy: 0.4082\n",
            "Epoch 31/50\n",
            "17/17 [==============================] - 12s 712ms/step - loss: 0.1654 - accuracy: 0.9373 - val_loss: 99.0749 - val_accuracy: 0.3301\n",
            "Epoch 32/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1778 - accuracy: 0.9283 - val_loss: 212.9857 - val_accuracy: 0.3262\n",
            "Epoch 33/50\n",
            "17/17 [==============================] - 12s 715ms/step - loss: 0.1790 - accuracy: 0.9269 - val_loss: 77.7974 - val_accuracy: 0.3379\n",
            "Epoch 34/50\n",
            "17/17 [==============================] - 13s 741ms/step - loss: 0.1592 - accuracy: 0.9366 - val_loss: 98.1570 - val_accuracy: 0.3027\n",
            "Epoch 35/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1392 - accuracy: 0.9435 - val_loss: 163.6089 - val_accuracy: 0.3438\n",
            "Epoch 36/50\n",
            "17/17 [==============================] - 12s 711ms/step - loss: 0.1404 - accuracy: 0.9453 - val_loss: 81.1555 - val_accuracy: 0.3535\n",
            "Epoch 37/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1527 - accuracy: 0.9407 - val_loss: 154.7835 - val_accuracy: 0.3379\n",
            "Epoch 38/50\n",
            "17/17 [==============================] - 12s 721ms/step - loss: 0.1745 - accuracy: 0.9343 - val_loss: 92.8442 - val_accuracy: 0.2969\n",
            "Epoch 39/50\n",
            "17/17 [==============================] - 12s 714ms/step - loss: 0.1447 - accuracy: 0.9426 - val_loss: 101.5163 - val_accuracy: 0.3672\n",
            "Epoch 40/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1549 - accuracy: 0.9361 - val_loss: 112.8996 - val_accuracy: 0.2988\n",
            "Epoch 41/50\n",
            "17/17 [==============================] - 12s 718ms/step - loss: 0.1483 - accuracy: 0.9444 - val_loss: 152.9544 - val_accuracy: 0.3203\n",
            "Epoch 42/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1446 - accuracy: 0.9409 - val_loss: 180.4522 - val_accuracy: 0.2734\n",
            "Epoch 43/50\n",
            "17/17 [==============================] - 12s 721ms/step - loss: 0.1478 - accuracy: 0.9453 - val_loss: 131.9274 - val_accuracy: 0.2969\n",
            "Epoch 44/50\n",
            "17/17 [==============================] - 12s 715ms/step - loss: 0.1505 - accuracy: 0.9407 - val_loss: 136.6123 - val_accuracy: 0.3203\n",
            "Epoch 45/50\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1271 - accuracy: 0.9550 - val_loss: 264.3119 - val_accuracy: 0.3125\n",
            "Epoch 46/50\n",
            "17/17 [==============================] - 12s 714ms/step - loss: 0.1171 - accuracy: 0.9554 - val_loss: 204.7972 - val_accuracy: 0.2852\n",
            "Epoch 47/50\n",
            " 2/17 [==>...........................] - ETA: 4s - loss: 0.1609 - accuracy: 0.9375"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-604cf5f7be2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                    \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                    callbacks=[es])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 536\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 536\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    807\u001b[0m   \"\"\"\n\u001b[1;32m    808\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrepresentable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     \"\"\"\n\u001b[0;32m--> 938\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5Nj03hx5DQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YofamALVCrtt",
        "colab_type": "text"
      },
      "source": [
        "# vgg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzO1JHNJ1D6I",
        "colab_type": "text"
      },
      "source": [
        "## basic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTe9kVl2CuIT",
        "colab_type": "code",
        "outputId": "26e88563-dda1-4f09-a0b6-7fb0b3f6f2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "base_vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3),pooling=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 5s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACJAJcwtojIV",
        "colab_type": "code",
        "outputId": "bfbebb02-535e-47cf-a941-9820b216c056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        }
      },
      "source": [
        "base_vgg.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUTuVUnlC2-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add a global spatial average pooling layer\n",
        "x = base_vgg.output\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# and a logits layer -- let's say we have 3 classes\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "base_vgg_model = Model(inputs=base_vgg.input, outputs=predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HBbHXx_m-Db",
        "colab_type": "code",
        "outputId": "4368932b-344f-46c2-cbb7-1b4818d0e726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "base_vgg_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 15,504,195\n",
            "Trainable params: 15,504,195\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f99YWWgxC99k",
        "colab_type": "code",
        "outputId": "656fdc5c-4fce-492a-8423-845f30ce6063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "for layer in base_vgg.layers:\n",
        "    print(layer.name)\n",
        "    layer.trainable = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_1\n",
            "block1_conv1\n",
            "block1_conv2\n",
            "block1_pool\n",
            "block2_conv1\n",
            "block2_conv2\n",
            "block2_pool\n",
            "block3_conv1\n",
            "block3_conv2\n",
            "block3_conv3\n",
            "block3_pool\n",
            "block4_conv1\n",
            "block4_conv2\n",
            "block4_conv3\n",
            "block4_pool\n",
            "block5_conv1\n",
            "block5_conv2\n",
            "block5_conv3\n",
            "block5_pool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2P0-ZTaDAPW",
        "colab_type": "code",
        "outputId": "7ebdb658-38e4-43c9-e7e2-5e4fc1945f38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "for layer in base_vgg_model.layers:\n",
        "    print(layer.name,' Trainable =',layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_1  Trainable = True\n",
            "block1_conv1  Trainable = True\n",
            "block1_conv2  Trainable = True\n",
            "block1_pool  Trainable = True\n",
            "block2_conv1  Trainable = True\n",
            "block2_conv2  Trainable = True\n",
            "block2_pool  Trainable = True\n",
            "block3_conv1  Trainable = True\n",
            "block3_conv2  Trainable = True\n",
            "block3_conv3  Trainable = True\n",
            "block3_pool  Trainable = True\n",
            "block4_conv1  Trainable = True\n",
            "block4_conv2  Trainable = True\n",
            "block4_conv3  Trainable = True\n",
            "block4_pool  Trainable = True\n",
            "block5_conv1  Trainable = True\n",
            "block5_conv2  Trainable = True\n",
            "block5_conv3  Trainable = True\n",
            "block5_pool  Trainable = True\n",
            "global_average_pooling2d_1  Trainable = True\n",
            "dense_4  Trainable = True\n",
            "dropout_3  Trainable = True\n",
            "dense_5  Trainable = True\n",
            "dropout_4  Trainable = True\n",
            "dense_6  Trainable = True\n",
            "dropout_5  Trainable = True\n",
            "dense_7  Trainable = True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOYp6zEBDDF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.RMSprop(lr=1e-4)\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "base_vgg_model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZilESfkQDI0b",
        "colab_type": "code",
        "outputId": "446a87fa-278d-4416-fe7c-5ba7b01d16cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "source": [
        "train_steps = int(len(df)/batch_size) #total trains set / batch_size\n",
        "val_steps = int(len(df_val)/batch_size)\n",
        "#epochs = 1\n",
        "\n",
        "print('train steps:',train_steps)\n",
        "print('val steps:',val_steps)\n",
        "epochs = 50\n",
        "\n",
        "### Train the model with validation \n",
        "vgg16_history = base_vgg_model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train steps: 70\n",
            "val steps: 10\n",
            "Train for 70 steps, validate for 10 steps\n",
            "Epoch 1/50\n",
            "70/70 [==============================] - 5050s 72s/step - loss: 1.2087 - accuracy: 0.3759 - val_loss: 0.9967 - val_accuracy: 0.6078\n",
            "Epoch 2/50\n",
            "70/70 [==============================] - 75s 1s/step - loss: 0.7894 - accuracy: 0.5924 - val_loss: 0.6629 - val_accuracy: 0.6609\n",
            "Epoch 3/50\n",
            "70/70 [==============================] - 86s 1s/step - loss: 0.6480 - accuracy: 0.6675 - val_loss: 0.5581 - val_accuracy: 0.7414\n",
            "Epoch 4/50\n",
            "70/70 [==============================] - 76s 1s/step - loss: 0.5768 - accuracy: 0.7189 - val_loss: 0.5109 - val_accuracy: 0.6914\n",
            "Epoch 5/50\n",
            "70/70 [==============================] - 85s 1s/step - loss: 0.5103 - accuracy: 0.7642 - val_loss: 0.4593 - val_accuracy: 0.8070\n",
            "Epoch 6/50\n",
            "70/70 [==============================] - 77s 1s/step - loss: 0.4722 - accuracy: 0.7878 - val_loss: 0.4447 - val_accuracy: 0.8094\n",
            "Epoch 7/50\n",
            "70/70 [==============================] - 75s 1s/step - loss: 0.4384 - accuracy: 0.8058 - val_loss: 0.4444 - val_accuracy: 0.8133\n",
            "Epoch 8/50\n",
            "70/70 [==============================] - 86s 1s/step - loss: 0.4045 - accuracy: 0.8209 - val_loss: 0.4597 - val_accuracy: 0.8055\n",
            "Epoch 9/50\n",
            "70/70 [==============================] - 76s 1s/step - loss: 0.4100 - accuracy: 0.8150 - val_loss: 0.4585 - val_accuracy: 0.8141\n",
            "Epoch 10/50\n",
            "70/70 [==============================] - 86s 1s/step - loss: 0.3707 - accuracy: 0.8308 - val_loss: 0.5166 - val_accuracy: 0.8023\n",
            "Epoch 11/50\n",
            "70/70 [==============================] - 76s 1s/step - loss: 0.3575 - accuracy: 0.8397 - val_loss: 0.7610 - val_accuracy: 0.7484\n",
            "Epoch 12/50\n",
            "70/70 [==============================] - 84s 1s/step - loss: 0.3491 - accuracy: 0.8450 - val_loss: 0.5142 - val_accuracy: 0.8016\n",
            "Epoch 13/50\n",
            "70/70 [==============================] - 78s 1s/step - loss: 0.3337 - accuracy: 0.8449 - val_loss: 0.5211 - val_accuracy: 0.8086\n",
            "Epoch 14/50\n",
            "70/70 [==============================] - 76s 1s/step - loss: 0.3250 - accuracy: 0.8503 - val_loss: 0.4994 - val_accuracy: 0.8023\n",
            "Epoch 15/50\n",
            "70/70 [==============================] - 86s 1s/step - loss: 0.2992 - accuracy: 0.8592 - val_loss: 0.5924 - val_accuracy: 0.7984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ExLpzmeQaS_",
        "colab_type": "code",
        "outputId": "3e9306f9-a37e-4df7-c532-eea2008b1b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "test_steps = int(len(df_test)/batch_size)\n",
        "\n",
        "metrics = base_vgg_model.evaluate(test_dataset,\n",
        "                   steps = test_steps)\n",
        "print(\"model accuracy:\",metrics[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9f37aea22d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m metrics = base_vgg_model.evaluate(test_dataset,\n\u001b[1;32m      4\u001b[0m                    steps = test_steps)\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCGpPP-l1J7_",
        "colab_type": "text"
      },
      "source": [
        "## 1st unfreezing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whAJkq6qDOfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_vgg_model.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in base_vgg_model.layers:\n",
        "    if layer.name == 'block5_conv1':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHCq5zulG67p",
        "colab_type": "code",
        "outputId": "30e183ee-7b60-42b1-f392-74d9dd8421ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "for layer in base_vgg_model.layers:\n",
        "    print(layer.name,layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_1 False\n",
            "block1_conv1 False\n",
            "block1_conv2 False\n",
            "block1_pool False\n",
            "block2_conv1 False\n",
            "block2_conv2 False\n",
            "block2_pool False\n",
            "block3_conv1 False\n",
            "block3_conv2 False\n",
            "block3_conv3 False\n",
            "block3_pool False\n",
            "block4_conv1 False\n",
            "block4_conv2 False\n",
            "block4_conv3 False\n",
            "block4_pool False\n",
            "block5_conv1 True\n",
            "block5_conv2 True\n",
            "block5_conv3 True\n",
            "block5_pool True\n",
            "global_average_pooling2d True\n",
            "dense True\n",
            "dropout True\n",
            "dense_1 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZuKehrVG9L3",
        "colab_type": "code",
        "outputId": "93ce7314-cb74-433f-a71a-e3ecf3a8a763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "\t\n",
        "es = EarlyStopping(monitor='accuracy', mode='max', verbose=1, patience=15)\n",
        "\n",
        "vgg_model_filepath = 'vgg16.h5'\n",
        "mcp = ModelCheckpoint(vgg_model_filepath, monitor='accuracy', save_best_only=True, mode='max')\n",
        "\n",
        "# Train the model with validation \n",
        "tuned_vgg_history = base_vgg_model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps,\n",
        "                   callbacks=[es, mcp])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 17 steps, validate for 2 steps\n",
            "Epoch 1/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.4264 - accuracy: 0.8118 - val_loss: 0.4985 - val_accuracy: 0.7871\n",
            "Epoch 2/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.3903 - accuracy: 0.8295 - val_loss: 0.5119 - val_accuracy: 0.7832\n",
            "Epoch 3/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.3957 - accuracy: 0.8284 - val_loss: 0.4664 - val_accuracy: 0.8027\n",
            "Epoch 4/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.3788 - accuracy: 0.8366 - val_loss: 0.5116 - val_accuracy: 0.7812\n",
            "Epoch 5/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.3772 - accuracy: 0.8339 - val_loss: 0.4974 - val_accuracy: 0.7812\n",
            "Epoch 6/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.3631 - accuracy: 0.8470 - val_loss: 0.4850 - val_accuracy: 0.7773\n",
            "Epoch 7/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.3543 - accuracy: 0.8502 - val_loss: 0.5181 - val_accuracy: 0.7930\n",
            "Epoch 8/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.3500 - accuracy: 0.8504 - val_loss: 0.5044 - val_accuracy: 0.7910\n",
            "Epoch 9/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.3379 - accuracy: 0.8559 - val_loss: 0.4972 - val_accuracy: 0.7793\n",
            "Epoch 10/200\n",
            "17/17 [==============================] - 19s 1s/step - loss: 0.3376 - accuracy: 0.8575 - val_loss: 0.5065 - val_accuracy: 0.7852\n",
            "Epoch 11/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.3226 - accuracy: 0.8670 - val_loss: 0.5279 - val_accuracy: 0.7695\n",
            "Epoch 12/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.3239 - accuracy: 0.8628 - val_loss: 0.4927 - val_accuracy: 0.7812\n",
            "Epoch 13/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.3167 - accuracy: 0.8672 - val_loss: 0.4894 - val_accuracy: 0.8008\n",
            "Epoch 14/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.3072 - accuracy: 0.8725 - val_loss: 0.5082 - val_accuracy: 0.7871\n",
            "Epoch 15/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.3056 - accuracy: 0.8745 - val_loss: 0.5067 - val_accuracy: 0.7949\n",
            "Epoch 16/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.2988 - accuracy: 0.8722 - val_loss: 0.5469 - val_accuracy: 0.7773\n",
            "Epoch 17/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2906 - accuracy: 0.8798 - val_loss: 0.5121 - val_accuracy: 0.7793\n",
            "Epoch 18/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2757 - accuracy: 0.8869 - val_loss: 0.5442 - val_accuracy: 0.7930\n",
            "Epoch 19/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.2799 - accuracy: 0.8849 - val_loss: 0.5074 - val_accuracy: 0.7910\n",
            "Epoch 20/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.2846 - accuracy: 0.8817 - val_loss: 0.5532 - val_accuracy: 0.7695\n",
            "Epoch 21/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.2575 - accuracy: 0.8957 - val_loss: 0.4972 - val_accuracy: 0.7832\n",
            "Epoch 22/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2588 - accuracy: 0.8991 - val_loss: 0.5310 - val_accuracy: 0.7832\n",
            "Epoch 23/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.2612 - accuracy: 0.8833 - val_loss: 0.5394 - val_accuracy: 0.7754\n",
            "Epoch 24/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.2573 - accuracy: 0.8959 - val_loss: 0.5451 - val_accuracy: 0.7871\n",
            "Epoch 25/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.2476 - accuracy: 0.8987 - val_loss: 0.5845 - val_accuracy: 0.7734\n",
            "Epoch 26/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2471 - accuracy: 0.9028 - val_loss: 0.5742 - val_accuracy: 0.7637\n",
            "Epoch 27/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.2461 - accuracy: 0.8987 - val_loss: 0.5689 - val_accuracy: 0.7852\n",
            "Epoch 28/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.2428 - accuracy: 0.9003 - val_loss: 0.5940 - val_accuracy: 0.7871\n",
            "Epoch 29/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.2411 - accuracy: 0.9014 - val_loss: 0.5433 - val_accuracy: 0.7871\n",
            "Epoch 30/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2353 - accuracy: 0.9035 - val_loss: 0.5878 - val_accuracy: 0.7695\n",
            "Epoch 31/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2256 - accuracy: 0.9088 - val_loss: 0.5501 - val_accuracy: 0.7812\n",
            "Epoch 32/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.2148 - accuracy: 0.9125 - val_loss: 0.5144 - val_accuracy: 0.7812\n",
            "Epoch 33/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2197 - accuracy: 0.9136 - val_loss: 0.6047 - val_accuracy: 0.7891\n",
            "Epoch 34/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2151 - accuracy: 0.9104 - val_loss: 0.5859 - val_accuracy: 0.7969\n",
            "Epoch 35/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.2088 - accuracy: 0.9187 - val_loss: 0.5262 - val_accuracy: 0.7793\n",
            "Epoch 36/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2043 - accuracy: 0.9205 - val_loss: 0.6117 - val_accuracy: 0.7637\n",
            "Epoch 37/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.2082 - accuracy: 0.9131 - val_loss: 0.6182 - val_accuracy: 0.7695\n",
            "Epoch 38/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1976 - accuracy: 0.9189 - val_loss: 0.6132 - val_accuracy: 0.8008\n",
            "Epoch 39/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.2033 - accuracy: 0.9193 - val_loss: 0.6001 - val_accuracy: 0.7637\n",
            "Epoch 40/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.2042 - accuracy: 0.9177 - val_loss: 0.5944 - val_accuracy: 0.7988\n",
            "Epoch 41/200\n",
            "17/17 [==============================] - 19s 1s/step - loss: 0.1880 - accuracy: 0.9274 - val_loss: 0.6483 - val_accuracy: 0.7734\n",
            "Epoch 42/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1910 - accuracy: 0.9200 - val_loss: 0.7141 - val_accuracy: 0.7559\n",
            "Epoch 43/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1821 - accuracy: 0.9297 - val_loss: 0.6395 - val_accuracy: 0.7773\n",
            "Epoch 44/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1870 - accuracy: 0.9237 - val_loss: 0.6242 - val_accuracy: 0.7715\n",
            "Epoch 45/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1869 - accuracy: 0.9281 - val_loss: 0.6063 - val_accuracy: 0.7734\n",
            "Epoch 46/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1785 - accuracy: 0.9295 - val_loss: 0.5476 - val_accuracy: 0.7910\n",
            "Epoch 47/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1742 - accuracy: 0.9301 - val_loss: 0.7089 - val_accuracy: 0.7695\n",
            "Epoch 48/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1760 - accuracy: 0.9258 - val_loss: 0.6498 - val_accuracy: 0.7832\n",
            "Epoch 49/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1660 - accuracy: 0.9375 - val_loss: 0.6856 - val_accuracy: 0.7715\n",
            "Epoch 50/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1715 - accuracy: 0.9308 - val_loss: 0.6621 - val_accuracy: 0.7637\n",
            "Epoch 51/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1618 - accuracy: 0.9393 - val_loss: 0.6969 - val_accuracy: 0.7617\n",
            "Epoch 52/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1675 - accuracy: 0.9327 - val_loss: 0.5898 - val_accuracy: 0.7695\n",
            "Epoch 53/200\n",
            "17/17 [==============================] - 25s 1s/step - loss: 0.1527 - accuracy: 0.9416 - val_loss: 0.6676 - val_accuracy: 0.7676\n",
            "Epoch 54/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1628 - accuracy: 0.9396 - val_loss: 0.6829 - val_accuracy: 0.7734\n",
            "Epoch 55/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1558 - accuracy: 0.9405 - val_loss: 0.6795 - val_accuracy: 0.7871\n",
            "Epoch 56/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1607 - accuracy: 0.9382 - val_loss: 0.7119 - val_accuracy: 0.7715\n",
            "Epoch 57/200\n",
            "17/17 [==============================] - 19s 1s/step - loss: 0.1519 - accuracy: 0.9423 - val_loss: 0.6378 - val_accuracy: 0.7656\n",
            "Epoch 58/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1554 - accuracy: 0.9396 - val_loss: 0.7030 - val_accuracy: 0.7734\n",
            "Epoch 59/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1528 - accuracy: 0.9386 - val_loss: 0.6244 - val_accuracy: 0.7969\n",
            "Epoch 60/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1563 - accuracy: 0.9370 - val_loss: 0.6633 - val_accuracy: 0.7676\n",
            "Epoch 61/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.1468 - accuracy: 0.9444 - val_loss: 0.7453 - val_accuracy: 0.7520\n",
            "Epoch 62/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1414 - accuracy: 0.9449 - val_loss: 0.7213 - val_accuracy: 0.7832\n",
            "Epoch 63/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1472 - accuracy: 0.9409 - val_loss: 0.6678 - val_accuracy: 0.8008\n",
            "Epoch 64/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1386 - accuracy: 0.9490 - val_loss: 0.7037 - val_accuracy: 0.7656\n",
            "Epoch 65/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1386 - accuracy: 0.9485 - val_loss: 0.7492 - val_accuracy: 0.7676\n",
            "Epoch 66/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.1328 - accuracy: 0.9529 - val_loss: 0.8169 - val_accuracy: 0.7617\n",
            "Epoch 67/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1305 - accuracy: 0.9515 - val_loss: 0.7497 - val_accuracy: 0.7617\n",
            "Epoch 68/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1316 - accuracy: 0.9488 - val_loss: 0.8522 - val_accuracy: 0.7305\n",
            "Epoch 69/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1350 - accuracy: 0.9455 - val_loss: 0.7270 - val_accuracy: 0.7734\n",
            "Epoch 70/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1313 - accuracy: 0.9511 - val_loss: 0.7023 - val_accuracy: 0.7832\n",
            "Epoch 71/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1344 - accuracy: 0.9481 - val_loss: 0.7644 - val_accuracy: 0.7676\n",
            "Epoch 72/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1195 - accuracy: 0.9545 - val_loss: 0.7257 - val_accuracy: 0.7949\n",
            "Epoch 73/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1253 - accuracy: 0.9490 - val_loss: 0.7671 - val_accuracy: 0.7910\n",
            "Epoch 74/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1378 - accuracy: 0.9442 - val_loss: 0.7050 - val_accuracy: 0.7930\n",
            "Epoch 75/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1231 - accuracy: 0.9552 - val_loss: 0.8207 - val_accuracy: 0.7695\n",
            "Epoch 76/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.1193 - accuracy: 0.9563 - val_loss: 0.8125 - val_accuracy: 0.7520\n",
            "Epoch 77/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1220 - accuracy: 0.9543 - val_loss: 0.7588 - val_accuracy: 0.7812\n",
            "Epoch 78/200\n",
            "17/17 [==============================] - 17s 1s/step - loss: 0.1253 - accuracy: 0.9517 - val_loss: 0.8292 - val_accuracy: 0.7520\n",
            "Epoch 79/200\n",
            "17/17 [==============================] - 24s 1s/step - loss: 0.1145 - accuracy: 0.9570 - val_loss: 0.7949 - val_accuracy: 0.7656\n",
            "Epoch 80/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1126 - accuracy: 0.9584 - val_loss: 0.7197 - val_accuracy: 0.7930\n",
            "Epoch 81/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1161 - accuracy: 0.9591 - val_loss: 0.7948 - val_accuracy: 0.7812\n",
            "Epoch 82/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1132 - accuracy: 0.9563 - val_loss: 0.8364 - val_accuracy: 0.7676\n",
            "Epoch 83/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1068 - accuracy: 0.9598 - val_loss: 0.7539 - val_accuracy: 0.7793\n",
            "Epoch 84/200\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1121 - accuracy: 0.9577 - val_loss: 0.8015 - val_accuracy: 0.7734\n",
            "Epoch 85/200\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1050 - accuracy: 0.9632 - val_loss: 0.8746 - val_accuracy: 0.7637\n",
            "Epoch 86/200\n",
            "17/17 [==============================] - 16s 950ms/step - loss: 0.1111 - accuracy: 0.9577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-5940296f0000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                    \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                    callbacks=[es, mcp])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                       total_epochs=1)\n\u001b[0m\u001b[1;32m    387\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m    388\u001b[0m                                  prefix='val_')\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    588\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2293\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1555\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1556\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1635\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1637\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1638\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1639\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRA10E4Zhjwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_vgg_model.save(cwd + '/models/vgg_tuned.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTJmv9z6iQyI",
        "colab_type": "code",
        "outputId": "db695887-a700-4861-a21f-e47910e174ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "best_vgg_model = load_model(cwd + '/models/vgg_tuned.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whH3xT9KT1Ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(cwd + '/trainHistoryDict/vgg_history.p', 'wb') as file_pi:\n",
        "        pickle.dump(tuned_vgg_history.history, file_pi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwzmuBOAHCFn",
        "colab_type": "code",
        "outputId": "f63c0306-9a32-4f51-ec3b-9b1fe8fb6bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "best_vgg_model = load_model(vgg_model_filepath)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgDMCMdBHsqD",
        "colab_type": "code",
        "outputId": "4a6e203a-c8d1-45c2-decc-4a204ece1e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "test_steps = int(len(df_test)/batch_size)\n",
        "\n",
        "metrics = best_vgg_model.evaluate(test_dataset,\n",
        "                   steps = test_steps)\n",
        "print(\"model accuracy:\",metrics[1])\n",
        "\n",
        "metrics = base_vgg_model.evaluate(test_dataset,\n",
        "                   steps = test_steps)\n",
        "print(\"model accuracy:\",metrics[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 3s 1s/step - loss: 0.8058 - accuracy: 0.7754\n",
            "model accuracy: 0.7753906\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.8219 - accuracy: 0.7852\n",
            "model accuracy: 0.78515625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc8aG9-Yi8rR",
        "colab_type": "code",
        "outputId": "1d0e614c-31dc-474d-b866-182d0c7b2518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "11584/15000"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7722666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvuqRZujjzE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kn5SntpGf9X",
        "colab_type": "text"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFoqrIi2Glic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jce_OdpHOJuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_HEIGHT = 200\n",
        "IMG_WIDTH = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51pKP7x5GtBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(3, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3sm56JNGw1g",
        "colab_type": "code",
        "outputId": "040f0bd6-cd0f-4684-dbd4-57df9fcf466d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 200, 200, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 100, 100, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 100, 100, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 50, 50, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 50, 50, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 25, 25, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 40000)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               20480512  \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 20,505,635\n",
            "Trainable params: 20,505,635\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "camw-Zu0kylR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.RMSprop(lr=1e-4)\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bcc3f231-7d9a-4664-b5d1-0dbc68bb7f6f",
        "id": "YLzHTDQNS27B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train_steps = int(len(df)/batch_size) #total trains set / batch_size\n",
        "val_steps = int(len(df_val)/batch_size)\n",
        "\n",
        "\n",
        "print('train steps:',train_steps)\n",
        "print('val steps:',val_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train steps: 14\n",
            "val steps: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj_1BALuKjcQ",
        "colab_type": "code",
        "outputId": "1e29316b-3ad9-4306-8c52-5f01e17a1c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "### Train the model with validation \n",
        "inception_history = model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 14 steps, validate for 2 steps\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 17s 1s/step - loss: 5.8621 - accuracy: 0.3304 - val_loss: 6.0557 - val_accuracy: 0.3281\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 10s 695ms/step - loss: 5.6985 - accuracy: 0.3331 - val_loss: 6.0557 - val_accuracy: 0.3281\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 16s 1s/step - loss: 5.8707 - accuracy: 0.3379 - val_loss: 5.9352 - val_accuracy: 0.3340\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 10s 699ms/step - loss: 5.6770 - accuracy: 0.3345 - val_loss: 5.9051 - val_accuracy: 0.3340\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 16s 1s/step - loss: 5.8362 - accuracy: 0.3290 - val_loss: 5.8750 - val_accuracy: 0.3418\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 10s 692ms/step - loss: 5.6985 - accuracy: 0.3284 - val_loss: 5.9352 - val_accuracy: 0.3379\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 10s 698ms/step - loss: 5.8061 - accuracy: 0.3373 - val_loss: 5.9051 - val_accuracy: 0.3340\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 16s 1s/step - loss: 5.8448 - accuracy: 0.3337 - val_loss: 5.9955 - val_accuracy: 0.3379\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 10s 700ms/step - loss: 5.7588 - accuracy: 0.3318 - val_loss: 6.1160 - val_accuracy: 0.3301\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 16s 1s/step - loss: 5.7502 - accuracy: 0.3382 - val_loss: 5.9653 - val_accuracy: 0.3281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIh0qEUXkk9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxK0uplZu3lh",
        "colab_type": "text"
      },
      "source": [
        "# Resnet 50\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80QQfQfwvdcm",
        "colab_type": "code",
        "outputId": "c5812369-7544-4495-b8d2-0aaecf566b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((32, 200, 200, 3), (32, 3)), types: (tf.float32, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tzymGyBu8CS",
        "colab_type": "code",
        "outputId": "fbfc57bf-ecb3-4394-9478-be9b855e1f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "input_shapes = (224,224,3)\n",
        "\n",
        "resnet_50 = tf.keras.applications.ResNet50(\n",
        "    include_top=True,\n",
        "    weights='imagenet',\n",
        "    input_shape=input_shapes,\n",
        "    pooling=None\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102973440/102967424 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC6H5p6TIEqP",
        "colab_type": "code",
        "outputId": "1d969552-02d1-4132-e5be-077c51a49ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "resnet_50.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "probs (Dense)                   (None, 1000)         2049000     avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 25,636,712\n",
            "Trainable params: 25,583,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkGNAFuB36AK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = resnet_50.output\n",
        "\n",
        "# x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "#x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# and a logits layer -- let's say we have 3 classes\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs = resnet_50.input, outputs = predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xTcZpl65W7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 3\n",
        "x = resnet_50.output\n",
        "# x = GlobalAveragePooling2D()(x)\n",
        "# x = Dropout(0.5)(x)\n",
        "predictions = Dense(num_classes, activation= 'softmax')(x)\n",
        "model = Model(inputs = resnet_50.input, outputs = predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM85-I7iyiwc",
        "colab_type": "code",
        "outputId": "6752d8c3-0780-4a8b-e40f-1ea64c81c411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "probs (Dense)                   (None, 1000)         2049000     avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          512512      probs[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 3)            1539        dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 26,676,075\n",
            "Trainable params: 26,622,955\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1EOiKpeylYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5d893213-0660-42bd-ec25-6cdb2f522607",
        "id": "a6ySjCVIy8ub",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train_steps = int(len(df)/batch_size) #total trains set / batch_size\n",
        "val_steps = int(len(df_val)/batch_size)\n",
        "\n",
        "\n",
        "print('train steps:',train_steps)\n",
        "print('val steps:',val_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train steps: 23\n",
            "val steps: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Yd_-FG6_8g",
        "colab_type": "code",
        "outputId": "cb71a01f-3a5a-4b16-b78d-8866df53e5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "valid_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 224, 224, 3), (256, 3)), types: (tf.float32, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahBpPH0OIne2",
        "colab_type": "code",
        "outputId": "55b2129e-6239-4d14-b2a5-b68e2195cc60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "#Train the model with validation\n",
        "history = model.fit( train_dataset, steps_per_epoch = train_steps, epochs = epochs, validation_data = valid_dataset, validation_steps = val_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 23 steps, validate for 3 steps\n",
            "Epoch 1/10\n",
            " 1/23 [>.............................] - ETA: 2:30:05"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-ef145d3459d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Train the model with validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    581\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2293\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1555\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1556\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1635\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1637\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1638\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1639\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[256,512,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_2/conv3_block3_3_bn/FusedBatchNormV3 (defined at <ipython-input-79-ef145d3459d9>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\tEncountered when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors. [Op:__inference_distributed_function_60189]\n\nFunction call stack:\ndistributed_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKl53seMzK8y",
        "colab_type": "code",
        "outputId": "beb44edd-4e92-4ebf-8ea4-ccabe90b93d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f23fabb4e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU5fX48c/JThKWAGHfEWUVWZSt\nKgL2q9atWkDrSl3qAqK2tda2Sq3tt9Yuaqu22LpvVdQW/Wr7E0GxIirggoALIktYQxICIWQ/vz+e\nyZDEJEzCzNyZuef9es0rM3fu3DkzSZ5zl+c5j6gqxhhj/CvJ6wCMMcZ4yxKBMcb4nCUCY4zxOUsE\nxhjjc5YIjDHG5ywRGGOMz0UsEYjIQyKyS0Q+aeJ5EZF7RWS9iHwsIqMjFYsxxpimRfKI4BHglGae\nPxUYFLhdCTwQwViMMcY0IWKJQFWXAoXNrHIW8Jg6y4EOItI9UvEYY4xpXIqH790T2FLncV5g2faG\nK4rIlbijBrKyssYMHjw4KgEaY0yiWLly5W5VzW3sOS8TQchUdT4wH2Ds2LG6YsUKjyMyxpj4IiKb\nmnrOy15DW4HedR73CiwzxhgTRV4mgoXAxYHeQ+OBYlX92mkhY4wxkRWxU0Mi8jQwGegsInnAbUAq\ngKr+BXgFOA1YD5QCsyIVizHGmKZFLBGo6vmHeF6BayP1/sYYY0JjI4uNMcbnLBEYY4zPWSIwxhif\ns0RgjDE+Z4nAGGN8Li5GFhtjTCKpqq5hf3k1e8sqKSmvcreyqoOPy9yyfWXuVlLuls+a2J9pQ7uG\nPR5LBMYYE6LaBnxfeWWggXaN9r7yKvaVVTbZgJcEHu8L3D9QWX3I90oSyE5PoW1GKm0zUshOT6Fa\nNSKfyxKBMSamlJRXsaWwlM2FpRSXVlJVo1TX1FBdo4H7SrUq1dXucY3WWR64VdXUUF0D1TU1bp06\nr234uN5rNPCa6vrbPVBRzb5WNODZ6SlkZ6SQk5VG746ZwQa97nNtax8Hn3O3NqnJiEgUvnFLBMaY\nKKuuUXbsLWNzQWmwwa+9bSkspWB/RYu2lySQkpREUpL7mZwkwVtKkpAkQkpyYJkElifXuR94bWZK\nytdfG3hNZlpyoBFPrdN4pzRovF3jnpkWvQY8XCwRGGPCbm9ZJVsKGzb0B9hSWEpeUSmV1QdPcSQn\nCT07tKFPx0y+OawbfTpmBm85WamkJie5xjxJSK7XgLuf8dboxiJLBMaYFquqrmF7cVm9vfnNhaXk\nBX4WlVbWW79DZip9OmYytEc7Thlev7Hv3j6DlGTrwOglSwTGmEYVl1Z+raGv3cPfuucA1TUH9+pT\nk4VeOZn0ymnDaSO6Bxv53oFb+zapHn4ScyiWCIzxmZoapWB/BTv3lrFzbxk79paxs7iMnXvL3f29\nZWzbc4C9ZVX1XtcpcMHzmN4dOHNkj2BD36dTJt3aZZCcZKdo4pUlAmMSSEl5FTuKy9hV28DvLXeN\nfXEZO/e5Bn/XvnKqaup3Q0wS6JydTrf2GfTKyeTYfh3p2ynQ0Aca/Ox0ay4Slf1mjYkDldU15O9z\ne+y7Ag37jr3lwQbfLS+npLzqa69tm5FC13YZdGuXwYSBnenazjX4Xdpm0K29W945O83O0/uYJQJj\nPFRWWc3uknIKSioo2F9O/r7yg6doAnvxO4rLKdhfTsOxRKnJEmzMB3dry4lH5gYb/K7tMujaLp2u\n7TLIsj15cwj2F2JMGKkqew9UsXt/Obv3lVOwv4LdJeXsLqkINPjufu3PxvbgwZ2P79Iug27t0hnR\ns32wwa9t3Lu1yyAnM40kOy9vwsASgTGHUFldQ9H+CvIDe+676/wMNvD7y9m9z+3V1+0jX0sEOmam\n0Sk7jc7Z6Yzo1YHOgfuds9PolJVO57bpgQSQTnpKsgef1PiVJQKTMDRQEqCiqsbdqt3P8jqPKwPL\ngsuD61RTWFJBQbDBP7jn3rBPfK205CTXiGenk5udzpBu7egUaNhdA58ebPhzMlPtHLyJWZYITNRV\nVtewubCUDfn7+Wp3CSVlVZTXaaDrNuL17je2rO7j6pqvnUdvqbYZKcG99CNysxk/oGNwb71zVlpw\nr71z23TapqfYqFaTECwRmIjZU1rBl/klfJm/ny/zS9gQ+Lm5oPRr3RfTUpJIT04iNSWJtOQk0lIC\ntzr3s9NTSMt091MbrJPeyPq166Q3s830wHrpKcl0yEwlI9VOyRj/sURgDktVdQ1big7w5a4SNuwu\n4ctd+93P/P0U1ikelpos9O2UxaAu2fzPsG4MzM1mQG4WAztn066N7Vkb4yVLBCYkxaWVfLm7JNDg\n7w/+3FSwv97F0U5ZaQzIzeKbQ7u6hj43mwG52fTOaWPnyI2JUZYITFBVdQ15RQfq79kHfu4uObh3\nn5Ik9O2UyYDcbKYNOdjgD8zNokNmmoefwBjTGpYIEpyqUlZZQ2lFFaUV1RyorKa0oprSiiq27ymr\nd+5+U0EpFdU1wdfmZKYyMDebKYO7BPfsB+Zm0btjJqm2d29MwrBEEANqajTYQB+oqKa0surg/UCj\nXXvfrff150sr6r++7vrN9aRJThL6dnR79wcb/CwG5GbTMcv27o3xA0sEUXCgopqP8/awcnMRqzYV\nsbmwlP3lBxv1ssqaQ2+kjuQkITM1mTZpyWSmJdMmLSU4g1KXtun1lrn7yWSmJpOZllLnNcl0aZtB\nn46ZpKXY3r0xfmaJIAK27TnAqs1FrNzkGv412/YGu0sO6JzFoK7ZZNVrlOs02oEGO9iAB28Hl6Ul\nJ1kvG2NM2FgiOEyV1TWs3baXlZuKgnv824vLAMhITWJkrw5cecIAxvTNYVSfHDvdYoyJOZYIWqig\npJxVm/e4vf3NRXyctyd4aqdnhzaM7deR0X06MKZvDkO6t7OLqsaYmGeJoBk1NcoXu0rc3n6g4f9q\n937AdaEc1rM93z2uL2P65jC6bwe6t2/jccTGGNNylgjq2FdWyYdbavf29/DB5iL2Babr65SVxui+\nOcw8tjdj+uYwomd7K0dgjEkIvk0EqsrmwtLg3v7KTUV8tnMfqq5k8FFd23LmyB6M7pPDmL459O2U\naRdojTEJyTeJoKyymtVbi1lV5zRP7WjZtukpHNOnA6cM78aYvjkc07sDbTNSPY7YGGOiwzeJ4C9v\nfsndi74AoH/nLE48skvw3P6gLm1JtpmejDE+FdFEICKnAPcAycDfVPU3DZ7vAzwKdAisc7OqvhKJ\nWM4Y2YNhPdozuk8HOmWnR+ItjDEmLkUsEYhIMnAfcDKQB7wvIgtVdW2d1X4GPKuqD4jIUOAVoF8k\n4nFF0bIjsWljjIlrkezkfhywXlU3qGoF8AxwVoN1FGgXuN8e2BbBeIwxxjQikomgJ7ClzuO8wLK6\n5gEXikge7mhgTmMbEpErRWSFiKzIz8+PRKzGGONbXg97PR94RFV7AacBj4vI12JS1fmqOlZVx+bm\n5kY9SGOMSWSRTARbgd51HvcKLKvrMuBZAFV9B8gAOkcwJmOMMQ1EMhG8DwwSkf4ikgacByxssM5m\nYCqAiAzBJQI792OMMVEUsUSgqlXAbOA/wDpc76A1InK7iJwZWO0HwBUi8hHwNHCpanPTqBhjjAm3\niI4jCIwJeKXBslvr3F8LTIpkDMYYY5rn9cViY4wxHrNEYIwxPmeJwBhjfM4SgTHG+JwlAmOM8TlL\nBMYY43OWCIwxxucsERhjjM9ZIjDGGJ+zROBXqlBd6XUUxpgY4Js5i00dZXvhkdNg1zrofCR0GQpd\nh0KXYe5n+94gNoezMX5hicBvVOFf18DOtXDs5VC0Eba8C58sOLhOejvoMiSQIIYdTBRtcjwL2xgT\nOZYI/GbZvbDuJfjmr2Di7IPLy4rdEcLONbBrrUsUa16AlQ8fXKdtj8CRQ50EkXsUpKRH/3MYY8LG\nEoGffLUUFs2DoWfDhGvrP5fRHvqMd7daqrB3WyAx1EkQXy2F6gq3jiRDpyPqn1rqMhQ69IWkGLkE\nVV0FB4rgQCGUFgZ+FtS5XwiVB2DaPOjQ+1BbMybhWCLwi+Kt8Nws6DQIzvpzaNcARKB9T3cbdPLB\n5dWVUPAl7FrjEsOutbB1Fax58eA6admQO7hBghgGWZ0O73NUlh1svEsLGjTuDe8Hni8rbnp7yemQ\n2RH27wZJgnMfPLz4jGmJmmo4sKeZv+naHZYid//EH8Pwc8IehiUCP6iqgOcugaoymPkEpLc9vO0l\np0KXwe42/NyDy8v3wa5P6yeIdS/DqscOrpPdtf6ppS5D3PIDhVBa1Mjeet1/hEKo3N90XGnZ0KYj\nZOZAZifI6eca+cxOgeWBW5s6P9OyXMJ77TZ4+x74xvUuNj97+x745Hm4bBGkpHkdTfyoqmj8aLO0\n4ODfb8OG/sAeoIm5uJJS6v/tdh7kjtwjwBKBH/znFsh7H2Y8BrlHRu590ttC72PdrZYqlOyEnZ8c\nTA4718B7D0J1edPbyuhwsBFv2901zpmd3AXrYGPeqX7DfjjXKibNhRUPweJfwflPtX478W7fTnjj\nN1BZ6o7wRs70OiLvFW2EvBVNn1asvV9R0vQ2UjPr76S07/31nZSGOyvpbaPWe88SQaL76Bl4/0GY\nOAeGnhX99xeBtt3c7YhpB5dXV0HhBsj/NLDnU+cfIaMDJEf5TzOzI0y8Dpbc4f7pe42N7vvHijfv\ndNd/2veBZX+Co2f4uyvxgSL4ywlQXuf0Ynr7gw16Vq7rMBFs0HMa30lJbePdZwiBJYJEtmM1vHQ9\n9Dseps7zOpr6klPc0Ukkj1BaavxV8O5f4PXb4ZKFXkcTfQVfwspHYOz3oPtIWDgbvnoTBkz2ODAP\nrXjIJYELnnffSZsO7tRogomRbh0m7A7sgX9c5P5wv/NQ9Pew41F6Wzj+B67x2/CG19FE3+JfQkoG\nnHiTOxLI6uKOCvyqsgyW/wUGToFB0yA7NyGTAFgiSEw1NfDiVVCc564LZHfxOqL4MfZ70K6XOyrQ\nJi7iJaKtK901gYmz3d9LSjqMuxLWL3LXdvzoo6dh/y6YdL3XkUScJYJE9N/fw+evwin/C72P8zqa\n+JKaAZN/7BrGz17xOproUHW9pjI7w4Q6gwzHXuYucr7zZ+9i80pNtRt82f0Y6H+C19FEnCWCRLP+\nddfz5eiZroSEabmR34WOA2HxHa5BSHRfvg4b33KnhDLaHVye2RFGXQgfPwt7t3sXnxc+fdl1ZvjG\n9b64WG6JIJEUbYLnL3P980+/2xd/wBGRnAJTfuq6un7yvNfRRFZNjRtt3qEvjJn19efHXw1aDe/9\nNeqheUYV/ns35PSHIWd6HU1UWCJIFJVl8OzF7h975uOQlul1RPFt6Leh6whY8qvELtf9yfOud9mU\nnzc+eKzjABhyRqD3TDP95BPJxrdg2yrX5Top2etoosISQaJ49Uew/UP49l+g00Cvo4l/SUkw9edu\nMNEHj3sdTWRUlcPi26HbiPojxBuaMMeV6fjgiejF5qW373HjA475rteRRI0lgkSw6jF3O/6HMPg0\nr6NJHIO+Cb3HwZu/dUXpEs2Kh2HPZpj2i+YLBPY+FnqPh+X3uYGAiWzHatdTatz3Y34QWDhZIoh3\n2z6A//shDDgJTrrF62gSiwhMvRX2bYf3/+Z1NOFVtheW/tb1iBk45dDrT5zjksa6BB9o9/a9kJrl\nekz5iCWCeFZaCP+42PX7PvfvvjmfGVX9vuEayrf+4BrPRPHOn13NnGnzQutUcNSprifVsj8l7viK\nok3umsmYS12PKR+xRBCvaqrh+cuhZAfMePTwyzubpk291RUVe+c+ryMJj307YdmfYdi3oeeY0F6T\nlOzmsNi2CjYti2x8Xll+v0uKE67xOpKos0QQr974jev/fdpdof8zm9bpMcp1I3znz7C/wOtoDt/S\n37rKr1N+3rLXjTzfFVNLxAFmpYXuOtuI6dC+l9fRRJ0lgnj02b/dP/OoC2H0JV5H4w8n/dSVZv7v\nH7yO5PDUFpYbfUnLe5elZbpBip+9Aru/iEh4nnnvQff7nXid15F4whJBvCn4El640lVCPO13Nmgs\nWroMhqPPcw1G8Vavo2m9xXdAcpqb6ao1jr3CzeqWSEcFFaVuwNyg/3Ez6fmQJYJ4UlHqBo0lJcGM\nx33VvS0mTP4xaA0svcvrSFpn6ypY84KrJ9S2a+u2kZ0Lx5wPHz4NJfnhjc8rHz7pLpx/I/GLyzUl\noolARE4Rkc9EZL2I3NzEOjNEZK2IrBERH08NdQiq8PINbnavc/4GOX29jsh/cvq5HiUfPO6OzOLN\nonnuHP/EOYe3nQmz3TWG9xNgfufqKldcrtex0GeC19F4JmKJQESSgfuAU4GhwPkiMrTBOoOAnwCT\nVHUY4N+UfCgr/g4fPwOTf+JqoxtvnPBDSEp1F+vjyZeL3TwLJ/yofmG51ug8CI46zY2tqCgNT3xe\nWftPNz5ikj+KyzUlkkcExwHrVXWDqlYAzwAN50q8ArhPVYsAVHVXBOOJX1veh1dvdiNdT/iR19H4\nW9tubtTp6ufc0Vk8qKlxZaY79HHzLYTDhNnudMpHT4dne15Qhbfvhk6BxOZjkUwEPYEtdR7nBZbV\ndSRwpIi8LSLLReSUxjYkIleKyAoRWZGfnyDnJUNVku+uC7TvCefMb74UgImOSXMhvZ278BoP1rwA\nOz4OFJZLD882+06EHqPd2Ip4LdW9YYkrKTHpOt//X3n96VOAQcBk4HzgQRHp0HAlVZ2vqmNVdWxu\nbm6UQ/RQdRUsmOUGM814HNrkeB2RATfqdNIc141yy/teR9O8qgo3BWXXETD8O+Hbroi71lD4JXz2\navi2G03/vRuyu7m5O3zukIlAROaISGtaoK1A7zqPewWW1ZUHLFTVSlX9CvgclxgMuMqQG99ycwt0\nP9rraExd4652M3otvt3rSJq38hFXQXXavPDv9Q45051uisd5jbd94K6ZjL86fEdJcSyUv4yuwPsi\n8mygF1CoV1TeBwaJSH8RSQPOAxpWrPon7mgAEemMO1W0IcTtJ7a1C1053LGXue56JrakZ7sLx18t\njd2J7sv3wZt3Qr/j4Yip4d9+cgqMvxa2LI/9I6OG3r7Hnd4b28hkPD50yESgqj/D7aX/HbgU+EJE\nfi0izQ5LVNUqYDbwH2Ad8KyqrhGR20Wkdtqf/wAFIrIWWAL8SFUTYAz/Ydr9BfzzGlc64pT/9Toa\n05Qxs2J7ovtlf4bS3XDyLyLXI2bUhZDRHt6Jo6OCwg2w9l8uCWS09zqamBDSsaKqKrAjcKsCcoAF\nIvLbQ7zuFVU9UlUHquqvAstuVdWFtdtV1RtVdaiqjlDVZw7r0ySC8hL4x4VutqgZj9lhayyL5Ynu\nS3a50b9Dz4psLar0bNcTad1LroGNB8v+DEkp7vSeAUK7RjBXRFYCvwXeBkao6tXAGKCZaY1Mi6nC\nwjmw+3P4zkO+LH4Vd2J1ovuld7nJdKbcGvn3Ou77IMmw/IHIv9fhKsl3I4mPngntunsdTcwI5Yig\nI3COqv6Pqj6nqpUAqloDnB7R6Pxm+QOuq9/UW2HAZK+jMaGoO9H96gVeR+MUbnBzDI+5BDofEfn3\na9cdjp7hprIsLYz8+x2O9/7qpuicNNfrSGJKKIngVSD42xWRdiIyDkBV10UqMN/ZtAz+389g8Olu\nlKOJH7UT3b/xa9dd02uHW1iuNSbMdtU7V/w9eu/ZUuUlrmjg4G+50dEmKJRE8ABQUudxSWCZCZd9\nO+C5S6Fjfzj7fl8PdY9LsTTR/bYP3Sxb469xo6CjpetQOGIavDsfKsui974tseoxKNtjO1qNCCUR\nSOBiMRA8JZQSuZB8proSnr3EdfWb+YT1YohXg77pJnj3eqL7RfOgTUc3WjbaJsyG/btc+Y1YU13p\nRkH3mQi9j/U6mpgTSiLYICLXiUhq4DYX6+sfPq/d6vphn/kn6DLE62hMa9VOdF+yw51+8MKXi13Z\nhBN+5M0OxYDJ7hTZsj+5+kax5JPnYW+er0tNNyeURHAVMBE3KjgPGAdcGcmgfGP1AjdP6rirYUQY\nh/8bb/SbBAOnulnMoj3RfU2NOxpo3weOvSy6712rtuzE7s9g/SJvYmiMqhtAljsEjjjZ62hiUigD\nynap6nmq2kVVu6rqd61KaBjsWue6ivYeD9/8pdfRmHCZ+nM4UBT9ie7XvgjbP3I9mLwcezL8HGjb\nw9X4jxVfvOZ6dU2a6/vick0JZRxBhohcKyL3i8hDtbdoBJewyordoLH0tjDjUUhO9ToiEy5eTHRf\nVQGv/xK6DneTr3spOdXV79n4lrtwHQvevhva9YThNuypKaGkx8eBbsD/AG/iisfti2RQCU3VlY8o\n/AqmPxLdnh0mOqI90f2qR6Hoq0BhueTovGdzxlwCaW1jY17jLe/DprdhwrVutL5pVCiJ4AhV/Tmw\nX1UfBb6Fu05gWuPNO+HTl+Gbd7ia7ibxRHOi+/KSOoXlYmTmuoz2Lhl88gLs2XLo9SPp7btdPKMv\n9jaOGBdKIqgM/NwjIsOB9kCXyIWUwN64E974Xxh5vjt8NokrONF9s+W4Dt8798H+fHc0EEvjT8Zd\n5X6++xfvYtj9BXz6f3DsFe40rGlSKIlgfmA+gp/hykivBe6MaFSJRtWN9nzj1642zVn3xdY/rQm/\n2onuV0VwovuSfHdRdsiZ0GtsZN6jtTr0dheOVz4CB/Z4E8Oye90I63Hf9+b940iziUBEkoC9qlqk\nqktVdUCg99BfoxRf/FN13fqW3uUOT8+6LzbO45rIO+FHriF6I0KlxGsLy02NQmG51pgwGypK3DWM\naNu3Az56BkZdANl2AuNQmk0EgVHEN0UplsSj6uoHvX23m2Dm9Hus+5qftO0K469y40V2fBLebRd+\n5QrLjb44duvm9DgG+p8Ay/8S/RpMyx+AmiqXjMwhhdIqLRKRH4pIbxHpWHuLeGTxThVe/bHrOXHc\n9+Fbv7ck4EcTr3MzYS35VXi3u+RXrqZ+NAvLtcaEObBvG6x5MXrvWbbXJckhZ0KnZufPMgGhtEwz\ngWuBpcDKwG1FJIOKezU18H83upK3E2bDqXfaNQG/isRE99s/cvV8JlwT+zX1j5gGuYNd2YlozeK2\n8mEo32ulplsglJHF/Ru5DYhGcHGppgZeus7tkXzjBtdN1JKAv4V7ovtF86BNTnw0dElJbmdo5+ro\nzO1cVe5OC/U/AXqOjvz7JYhQRhZf3NgtGsHFnZpq+Nc1rhTxCTfB1NssCZj6E91/ueTwtrXhDVdc\nzqvCcq1x9AzI6uKOCiLt42dh33YrNd1CoZwaOrbO7XhgHnBmcy/wpeoqePH78NHTbmTplJ9aEjAH\nhWOi+5oaeO02aN/bdT6IFynprgvnl6/DzjWRe5+aGtdltNsIGDglcu+TgEI5NTSnzu0KYDSQHfnQ\n4kh1JbxwuTtvO/U2ONE6WpkGaie637bKDXJqjbX/hO0fuh2N1IzwxhdpY78HqZmRLcb3+atuvu9J\n19tOWAu1phvLfqB/uAOJW1UVbnaxNS+66wHH3+h1RCZWjfwudDqidRPdV1fC4l9Cl2HuVEu8yewI\noy50p272bo/Me7x9D3ToA0PPjsz2E1go1wheEpGFgdvLwGdAFPuCxbCqcnj2Ylc76JQ7XS12Y5qS\nnOL25vPXtXyi+1WPuknpp82L3wGJ468GrXa96cJt0zuw5V3XXTXZJlBsqVC+sd/VuV8FbFLVvAjF\nEz8qy1wp6fWvuTECx17udUQmHgw9G7r9wZUbGfbt0Cpilpe4OlV9J8GgOJ5YpeMAGHKG61F3/A/C\nW//n7XvcFJ2jLgjfNn0klFNDm4F3VfVNVX0bKBCRfhGNKtZVlMLT57lZmM6415KACV1SEky5tWUT\n3S+/380FPO0X8X/ue+J1bj6OD54I3zZ3rXPXB8Z9H9KywrddHwklETwH1J2AtDqwzJ8q9sNTM1w3\nvrPvd+V2jWmJQSeHPtH9/t1ub3fIGYkx6XqvsdBnArxzv+tpFw7L/gQpbVyVUdMqoSSCFFUNFgoJ\n3PfnDA/l++CJ77iJLs6ZD8d81+uITDyqN9H9/ObXXfq7QGG526ITWzRMnAPFm2HdwsPfVvFWdwF6\n9MWQ1enwt+dToSSCfBEJjhsQkbOA3ZELKUaV7YUnznUXpM79W3z23DCxIzjR/R/dqZLGFG2E9//m\netvEamG51jjyVOg4MDxlJ5bf7+Z9mHBteGLzqVASwVXALSKyWUQ2Az8G/FXg+8AeePxs2LoSpj9s\nc5+a8DjURPeLA4XlJt8c3bgiLSnJNdzbVsGmZa3fzoEiN9/B8HMgp2/YwvOjUAaUfamq44GhwFBV\nnaiq6yMfWowoLYTHzoTtH8OMx2DoWV5HZBJFcKL7+9y1gLq2fwyrn3VdLtv18Ca+SBp5PmR2Oryy\nEysecvMdxEPNpRgXyjiCX4tIB1UtUdUSEckRkTuiEZzn9hfAo2fCrk/hvKdg8Le8jsgkmik/C0x0\n/8f6y1//RfwUlmuNtEx3cffzVyH/85a/vrLMzXMwcKorKWEOSyinhk5V1eBcc6paBJwWuZBiREk+\nPHo6FHwB5z8NR37T64hMIso9yu0d153ofsObrmvy8T+ANh28jS+Sjr0cUjLcnB0t9dHTrkvtN6y4\nXDiEkgiSRSS99oGItAHSm1k//u3bAY98y80C9d1/wBFTvY7IJLIT60x0rwqLbnMF6hK9O2R2Low8\nz00pWZIf+utqql1xuR6joN/xkYvPR0JJBE8Cr4vIZSJyOfAa4MEkpFGyd5tLAsV5cOECGDDZ64hM\nosvpC2NnuYnu//sH2PaBq14bb4XlWmPCbKguh/cfDP01n77sym1YcbmwCeVi8Z3AHcAQ4CjgP0Bi\nXqLfswUePg327YSLXoB+3/A6IuMXx//QTXT/+u3QZSgcPdPriKKj8yA46jR3aqyi9NDrq8J/7z5Y\nrsKERajVR3cCCkwHpgDrQnmRiJwiIp+JyHoRabIPnIicKyIqImNDjCf8ijbBI6dBaQFc9CL0Ge9Z\nKMaHaie6Bzd4LF4Ly7XGxDlwoBA+eurQ6258y3U7nTjHX99RhDVZdE5EjgTOD9x2A/8ARFVPCmXD\nIpIM3AecDOQB74vIQlVd29o2tAUAABH9SURBVGC9tsBc4N1WfYJwKNzgegeV74WL/2VT3BlvTP4J\nDDjJTbPoJ30mQM8xrhvtmFnNN/Bv3wNZue4Cuwmb5o4IPsXt/Z+uqt9Q1T/h6gyF6jhgvapuCJSl\neAZorBP+L4E7gbIWbDt8Cr6Eh7/laghd8pIlAeOdlHQYcKL/znuLuD38wg3w2atNr7djtetNNe4q\nSG0Tvfh8oLlEcA6wHVgiIg+KyFSgJX+hPYEtdR7nBZYFichooLeqNjtlk4hcKSIrRGRFfn4Lehcc\nSv7n7ppAdblLAt1Hhm/bxpjQDT7DTSrT3ACzt++FtGw4No6m6YwTTSYCVf2nqp4HDAaWANcDXUTk\nARE57E71IpIE/AH4waHWVdX5qjpWVcfm5uYe7ls7O9e6awJaA5f+H3QbHp7tGmNaLjkFxl8LW5bD\nlve+/nzRJvjkeRhzqRtoZ8IqlF5D+1X1KVU9A+gFfICrN3QoW4HedR73Ciyr1RYYDrwhIhuB8cDC\nqFww3rHaDRaTZJcEugyJ+FsaYw5h1IWQ0b7xo4Ll97tTSOOviX5cPtCiOYtVtSiwdx7KCKv3gUEi\n0l9E0oDzgGDdWVUtVtXOqtpPVfsBy4EzVXVFS2JqsW0fwqNnuBGNs16B3CMj+nbGmBClZ8PYy2Dd\nS+56Qa3SQlj1GIyYAe17Nv1602qtmbw+JKpaBczGjTtYBzyrqmtE5Pa6Za2jautKV0AuLdsdCXQa\n6EkYxpgmjPu+q7j6zv0Hl733oKvHNOk67+JKcBGd5VlVXwFeabDs1ibWnRzJWNjynptPoE0OXPqy\nuzBljIktbbu5uT4+fBJOusUdub/3VzjyFDuFG0EROyKIOQXrIbuLOx1kScCY2DVhtjsCWPF3lxBK\nC1w5CRMxET0iiCnHfBeGneOP+i3GxLOuQ+GIafDufPf/2us4G+kfYf45IgBLAsbEi4lzXJnpPZtd\nqWm/DbKLMv8cERhj4kf/E6H7MVBV5uY4NhFlicAYE3tE4MIXAHVzHJuIskRgjIlNWZ28jsA3LNUa\nY4zPWSIwxhifs0RgjDE+Z4nAGGN8zhKBMcb4nCUCY4zxOUsExhjjc5YIjDHG5ywRGGOMz1kiMMYY\nn7NEYIwxPmeJwBhjfM4SgTHG+JwlAmOM8TlLBMYY43OWCIwxxucsERhjjM9ZIjDGGJ+zRGCMMT5n\nicAYY3zOEoExxvicJQJjjPE5SwTGGONzlgiMMcbnLBEYY4zPWSIwxhifs0RgjDE+Z4nAGGN8LqKJ\nQEROEZHPRGS9iNzcyPM3ishaEflYRF4Xkb6RjMcYY8zXRSwRiEgycB9wKjAUOF9EhjZY7QNgrKoe\nDSwAfhupeIwxxjQukkcExwHrVXWDqlYAzwBn1V1BVZeoamng4XKgVwTjMcYY04hIJoKewJY6j/MC\ny5pyGfBqY0+IyJUiskJEVuTn54cxRGOMMTFxsVhELgTGAnc19ryqzlfVsao6Njc3N7rBGWNMgkuJ\n4La3Ar3rPO4VWFaPiEwDfgqcqKrlEYzHGGNMIyJ5RPA+MEhE+otIGnAesLDuCiIyCvgrcKaq7opg\nLMYYY5oQsUSgqlXAbOA/wDrgWVVdIyK3i8iZgdXuArKB50TkQxFZ2MTmjDHGREgkTw2hqq8ArzRY\ndmud+9Mi+f7GmMirrKwkLy+PsrIyr0MxQEZGBr169SI1NTXk10Q0ERhjEl9eXh5t27alX79+iIjX\n4fiaqlJQUEBeXh79+/cP+XUx0WvIGBO/ysrK6NSpkyWBGCAidOrUqcVHZ5YIjDGHzZJA7GjN78IS\ngTHG+JwlAmOM8TlLBMYYE6KqqiqvQ4gI6zVkjAmbX7y0hrXb9oZ1m0N7tOO2M4Ydcr2zzz6bLVu2\nUFZWxty5c7nyyiv597//zS233EJ1dTWdO3fm9ddfp6SkhDlz5rBixQpEhNtuu41zzz2X7OxsSkpK\nAFiwYAEvv/wyjzzyCJdeeikZGRl88MEHTJo0ifPOO4+5c+dSVlZGmzZtePjhhznqqKOorq7mxz/+\nMf/+979JSkriiiuuYNiwYdx7773885//BOC1117j/vvv58UXXwzrd3S4LBEYYxLCQw89RMeOHTlw\n4ADHHnssZ511FldccQVLly6lf//+FBYWAvDLX/6S9u3bs3r1agCKiooOue28vDyWLVtGcnIye/fu\n5a233iIlJYVFixZxyy238PzzzzN//nw2btzIhx9+SEpKCoWFheTk5HDNNdeQn59Pbm4uDz/8MN/7\n3vci+j20hiUCY0zYhLLnHin33ntvcE97y5YtzJ8/nxNOOCHYn75jx44ALFq0iGeeeSb4upycnENu\ne/r06SQnJwNQXFzMJZdcwhdffIGIUFlZGdzuVVddRUpKSr33u+iii3jiiSeYNWsW77zzDo899liY\nPnH4WCIwxsS9N954g0WLFvHOO++QmZnJ5MmTOeaYY/j0009D3kbdbpcN++FnZWUF7//85z/npJNO\n4sUXX2Tjxo1Mnjy52e3OmjWLM844g4yMDKZPnx5MFLHELhYbY+JecXExOTk5ZGZm8umnn7J8+XLK\nyspYunQpX331FUDw1NDJJ5/MfffdF3xt7amhrl27sm7dOmpqapo9h19cXEzPnm5qlUceeSS4/OST\nT+avf/1r8IJy7fv16NGDHj16cMcddzBr1qzwfegwskRgjIl7p5xyClVVVQwZMoSbb76Z8ePHk5ub\ny/z58znnnHMYOXIkM2fOBOBnP/sZRUVFDB8+nJEjR7JkyRIAfvOb33D66aczceJEunfv3uR73XTT\nTfzkJz9h1KhR9XoRXX755fTp04ejjz6akSNH8tRTTwWfu+CCC+jduzdDhgyJ0DdweERVvY6hRcaO\nHasrVqzwOgxjTMC6detitoGLFbNnz2bUqFFcdtllUXm/xn4nIrJSVcc2tn7snawyxpgEMmbMGLKy\nsvj973/vdShNskRgjDERtHLlSq9DOCS7RmCMMT5nicAYY3zOEoExxvicJQJjjPE5SwTGGONzlgiM\nMb6SnZ3tdQgxx7qPGmPC59WbYcfq8G6z2wg49Tfh3WYMqKqqipm6Q3ZEYIyJazfffHO92kHz5s3j\njjvuYOrUqYwePZoRI0bwr3/9K6RtlZSUNPm6xx57LFg+4qKLLgJg586dfPvb32bkyJGMHDmSZcuW\nsXHjRoYPHx583e9+9zvmzZsHwOTJk7n++usZO3Ys99xzDy+99BLjxo1j1KhRTJs2jZ07dwbjmDVr\nFiNGjODoo4/m+eef56GHHuL6668PbvfBBx/khhtuaPX3Vo+qxtVtzJgxaoyJHWvXrvX0/VetWqUn\nnHBC8PGQIUN08+bNWlxcrKqq+fn5OnDgQK2pqVFV1aysrCa3VVlZ2ejrPvnkEx00aJDm5+erqmpB\nQYGqqs6YMUP/+Mc/qqpqVVWV7tmzR7/66isdNmxYcJt33XWX3nbbbaqqeuKJJ+rVV18dfK6wsDAY\n14MPPqg33nijqqredNNNOnfu3Hrr7du3TwcMGKAVFRWqqjphwgT9+OOPG/0cjf1OgBXaRLsaG8cl\nxhjTSqNGjWLXrl1s27aN/Px8cnJy6NatGzfccANLly4lKSmJrVu3snPnTrp169bstlSVW2655Wuv\nW7x4MdOnT6dz587AwbkGFi9eHJxfIDk5mfbt2x9yopva4nfgJryZOXMm27dvp6KiIjh3QlNzJkyZ\nMoWXX36ZIUOGUFlZyYgRI1r4bTXOEoExJu5Nnz6dBQsWsGPHDmbOnMmTTz5Jfn4+K1euJDU1lX79\n+n1tjoHGtPZ1daWkpFBTUxN83NzcBnPmzOHGG2/kzDPP5I033gieQmrK5Zdfzq9//WsGDx4c1pLW\ndo3AGBP3Zs6cyTPPPMOCBQuYPn06xcXFdOnShdTUVJYsWcKmTZtC2k5Tr5syZQrPPfccBQUFwMG5\nBqZOncoDDzwAQHV1NcXFxXTt2pVdu3ZRUFBAeXk5L7/8crPvVzu3waOPPhpc3tScCePGjWPLli08\n9dRTnH/++aF+PYdkicAYE/eGDRvGvn376NmzJ927d+eCCy5gxYoVjBgxgscee4zBgweHtJ2mXjds\n2DB++tOfcuKJJzJy5EhuvPFGAO655x6WLFnCiBEjGDNmDGvXriU1NZVbb72V4447jpNPPrnZ9543\nbx7Tp09nzJgxwdNO0PScCQAzZsxg0qRJIU2xGSqbj8AYc1hsPoLoOv3007nhhhuYOnVqk+u0dD4C\nOyIwxpg4sGfPHo488kjatGnTbBJoDbtYbIzxndWrVwfHAtRKT0/n3Xff9SiiQ+vQoQOff/55RLZt\nicAYc9hUFRHxOoyQjRgxgg8//NDrMCKiNaf77dSQMeawZGRkUFBQ0KoGyISXqlJQUEBGRkaLXmdH\nBMaYw9KrVy/y8vLIz8/3OhSDS8y9evVq0WssERhjDktqampwRKyJTxE9NSQip4jIZyKyXkRubuT5\ndBH5R+D5d0WkXyTjMcYY83URSwQikgzcB5wKDAXOF5GhDVa7DChS1SOAPwJ3RioeY4wxjYvkEcFx\nwHpV3aCqFcAzwFkN1jkLqB1XvQCYKvHU9cAYYxJAJK8R9AS21HmcB4xrah1VrRKRYqATsLvuSiJy\nJXBl4GGJiHzWypg6N9y2z9n3UZ99HwfZd1FfInwffZt6Ii4uFqvqfGD+4W5HRFY0NcTaj+z7qM++\nj4Psu6gv0b+PSJ4a2gr0rvO4V2BZo+uISArQHiiIYEzGGGMaiGQieB8YJCL9RSQNOA9Y2GCdhcAl\ngfvfARarjUoxxpioitipocA5/9nAf4Bk4CFVXSMit+OmTFsI/B14XETWA4W4ZBFJh316KcHY91Gf\nfR8H2XdRX0J/H3FXhtoYY0x4Wa0hY4zxOUsExhjjc75JBIcqd+EXItJbRJaIyFoRWSMic72OKRaI\nSLKIfCAiTU8w6xMi0kFEFojIpyKyTkQmeB2TV0TkhsD/ySci8rSItKysZ5zwRSIIsdyFX1QBP1DV\nocB44Foffxd1zQXWeR1EjLgH+LeqDgZG4tPvRUR6AtcBY1V1OK7TS6Q7tHjCF4mA0Mpd+IKqblfV\nVYH7+3D/5D29jcpbItIL+BbwN69j8ZqItAdOwPXoQ1UrVHWPt1F5KgVoExjnlAls8zieiPBLImis\n3IWvGz+AQLXXUUDszs8XHXcDNwE1XgcSA/oD+cDDgVNlfxORLK+D8oKqbgV+B2wGtgPFqvr/vI0q\nMvySCEwDIpINPA9cr6p7vY7HKyJyOrBLVVd6HUuMSAFGAw+o6ihgP+DLa2oikoM7c9Af6AFkiciF\n3kYVGX5JBKGUu/ANEUnFJYEnVfUFr+Px2CTgTBHZiDtlOEVEnvA2JE/lAXmqWnuUuACXGPxoGvCV\nquaraiXwAjDR45giwi+JIJRyF74QKPP9d2Cdqv7B63i8pqo/UdVeqtoP93exWFUTcq8vFKq6A9gi\nIkcFFk0F1noYkpc2A+NFJDPwfzOVBL1wHhfVRw9XU+UuPA7LK5OAi4DVIvJhYNktqvqKhzGZ2DIH\neDKw07QBmOVxPJ5Q1XdFZAGwCtfb7gMStNSElZgwxhif88upIWOMMU2wRGCMMT5nicAYY3zOEoEx\nxvicJQJjjPE5SwTGNCAi1SLyYZ1b2EbWikg/EfkkXNszJhx8MY7AmBY6oKrHeB2EMdFiRwTGhEhE\nNorIb0VktYi8JyJHBJb3E5HFIvKxiLwuIn0Cy7uKyIsi8lHgVlueIFlEHgzUuf9/ItLGsw9lDJYI\njGlMmwanhmbWea5YVUcAf8ZVLQX4E/Coqh4NPAncG1h+L/Cmqo7E1eupHc0+CLhPVYcBe4BzI/x5\njGmWjSw2pgERKVHV7EaWbwSmqOqGQOG+HaraSUR2A91VtTKwfLuqdhaRfKCXqpbX2UY/4DVVHRR4\n/GMgVVXviPwnM6ZxdkRgTMtoE/dborzO/WrsWp3xmCUCY1pmZp2f7wTuL+PgFIYXAG8F7r8OXA3B\nOZHbRytIY1rC9kSM+bo2dSqzgpu/t7YLaY6IfIzbqz8/sGwObkavH+Fm96qt1jkXmC8il+H2/K/G\nzXRlTEyxawTGhChwjWCsqu72OhZjwslODRljjM/ZEYExxvicHREYY4zPWSIwxhifs0RgjDE+Z4nA\nGGN8zhKBMcb43P8HBldKVYDN49kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8ZWvl348stX",
        "colab_type": "code",
        "outputId": "e34bbe6c-37d4-4c83-d681-e5e3bfe256a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 3])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f23fabc8da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOSoEiqIoPuf/A2T6Yf5Tpfi9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDsFkgem-rWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bu3k07D2yUn",
        "colab_type": "text"
      },
      "source": [
        "# vgg19\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA22cUu822Bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_vgg19 = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3),pooling=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5yme9c425Pq",
        "colab_type": "code",
        "outputId": "88e1f9cf-073c-427b-f6b7-f1404f06e303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        }
      },
      "source": [
        "base_vgg19.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 20,024,384\n",
            "Trainable params: 20,024,384\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WqpbIB1l3F14",
        "colab": {}
      },
      "source": [
        "# add a global spatial average pooling layer\n",
        "x = base_vgg19.output\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# and a logits layer -- let's say we have 3 classes\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "vgg_model = Model(inputs=base_vgg19.input, outputs=predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8BR5CD23KYZ",
        "colab_type": "code",
        "outputId": "a1b539e0-1c28-41d4-91e9-44bc94580cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vgg_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 20,813,891\n",
            "Trainable params: 20,813,891\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-ll2eAG3iJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.RMSprop(lr=1e-4)\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "vgg_model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e6602f5f-4bef-49da-ad1c-f4d79ac11a2f",
        "id": "vEnaJBD04A1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "for layer in vgg_model.layers:\n",
        "    print(layer.name,' Trainable =',layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_5  Trainable = True\n",
            "block1_conv1  Trainable = True\n",
            "block1_conv2  Trainable = True\n",
            "block1_pool  Trainable = True\n",
            "block2_conv1  Trainable = True\n",
            "block2_conv2  Trainable = True\n",
            "block2_pool  Trainable = True\n",
            "block3_conv1  Trainable = True\n",
            "block3_conv2  Trainable = True\n",
            "block3_conv3  Trainable = True\n",
            "block3_conv4  Trainable = True\n",
            "block3_pool  Trainable = True\n",
            "block4_conv1  Trainable = True\n",
            "block4_conv2  Trainable = True\n",
            "block4_conv3  Trainable = True\n",
            "block4_conv4  Trainable = True\n",
            "block4_pool  Trainable = True\n",
            "block5_conv1  Trainable = True\n",
            "block5_conv2  Trainable = True\n",
            "block5_conv3  Trainable = True\n",
            "block5_conv4  Trainable = True\n",
            "block5_pool  Trainable = True\n",
            "global_average_pooling2d_4  Trainable = True\n",
            "dense_17  Trainable = True\n",
            "dropout_13  Trainable = True\n",
            "dense_18  Trainable = True\n",
            "dropout_14  Trainable = True\n",
            "dense_19  Trainable = True\n",
            "dropout_15  Trainable = True\n",
            "dense_20  Trainable = True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "48ad6cda-b03a-4137-8df2-14b9e8bbf3af",
        "id": "-us0Zk-13x0U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "train_steps = int(len(df)/batch_size) #total trains set / batch_size\n",
        "val_steps = int(len(df_val)/batch_size)\n",
        "#epochs = 1\n",
        "\n",
        "print('train steps:',train_steps)\n",
        "print('val steps:',val_steps)\n",
        "epochs = 50\n",
        "\n",
        "### Train the model with validation \n",
        "vgg19_history = vgg_model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train steps: 70\n",
            "val steps: 10\n",
            "Train for 70 steps, validate for 10 steps\n",
            "Epoch 1/50\n",
            "70/70 [==============================] - 97s 1s/step - loss: 0.7232 - accuracy: 0.6167 - val_loss: 0.5910 - val_accuracy: 0.7078\n",
            "Epoch 2/50\n",
            "70/70 [==============================] - 86s 1s/step - loss: 0.5233 - accuracy: 0.7406 - val_loss: 0.5740 - val_accuracy: 0.7227\n",
            "Epoch 3/50\n",
            "70/70 [==============================] - 97s 1s/step - loss: 0.5062 - accuracy: 0.7625 - val_loss: 0.4841 - val_accuracy: 0.7703\n",
            "Epoch 4/50\n",
            "70/70 [==============================] - 85s 1s/step - loss: 0.4960 - accuracy: 0.7626 - val_loss: 0.5102 - val_accuracy: 0.7664\n",
            "Epoch 5/50\n",
            "70/70 [==============================] - 95s 1s/step - loss: 0.4792 - accuracy: 0.7719 - val_loss: 0.5271 - val_accuracy: 0.7422\n",
            "Epoch 6/50\n",
            "70/70 [==============================] - 87s 1s/step - loss: 0.4563 - accuracy: 0.7848 - val_loss: 0.4874 - val_accuracy: 0.7781\n",
            "Epoch 7/50\n",
            "70/70 [==============================] - 85s 1s/step - loss: 0.5044 - accuracy: 0.7547 - val_loss: 0.4996 - val_accuracy: 0.7531\n",
            "Epoch 8/50\n",
            "70/70 [==============================] - 96s 1s/step - loss: 0.4820 - accuracy: 0.7657 - val_loss: 0.5182 - val_accuracy: 0.7242\n",
            "Epoch 9/50\n",
            "70/70 [==============================] - 85s 1s/step - loss: 0.4759 - accuracy: 0.7673 - val_loss: 0.5024 - val_accuracy: 0.7508\n",
            "Epoch 10/50\n",
            "70/70 [==============================] - 95s 1s/step - loss: 0.4683 - accuracy: 0.7708 - val_loss: 0.4918 - val_accuracy: 0.7625\n",
            "Epoch 11/50\n",
            "70/70 [==============================] - 86s 1s/step - loss: 0.8988 - accuracy: 0.5725 - val_loss: 1.0987 - val_accuracy: 0.3352\n",
            "Epoch 12/50\n",
            "70/70 [==============================] - 94s 1s/step - loss: 1.0990 - accuracy: 0.3329 - val_loss: 1.0989 - val_accuracy: 0.3211\n",
            "Epoch 13/50\n",
            "70/70 [==============================] - 87s 1s/step - loss: 1.0989 - accuracy: 0.3285 - val_loss: 1.0986 - val_accuracy: 0.3344\n",
            "Epoch 14/50\n",
            "70/70 [==============================] - 85s 1s/step - loss: 1.0988 - accuracy: 0.3382 - val_loss: 1.0987 - val_accuracy: 0.3273\n",
            "Epoch 15/50\n",
            " 5/70 [=>............................] - ETA: 2:40 - loss: 1.0982 - accuracy: 0.3555"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-fd3283132b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                    \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                    validation_steps = val_steps)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    581\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2293\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1555\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1556\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1635\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1637\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1638\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1639\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bc05f3ff-e63f-4f24-8bfd-b944c832ede1",
        "id": "B-mG91h-EfOs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "test_steps = int(len(df_test)/batch_size)\n",
        "\n",
        "metrics = vgg_model.evaluate(test_dataset,\n",
        "                   steps = test_steps)\n",
        "print(\"model accuracy:\",metrics[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 663ms/step - loss: 0.9572 - accuracy: 0.7664\n",
            "model accuracy: 0.76640624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6wRnTaaEgmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8RfkALIfa81",
        "colab_type": "text"
      },
      "source": [
        "# vgg16\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfn-Zejpfe1d",
        "colab_type": "code",
        "outputId": "5bf95ab0-f9de-4e07-db96-84223c10ff98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "base_vgg16 = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3),pooling=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 5s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWS7SZLJftPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add a global spatial average pooling layer\n",
        "x = base_vgg16.output\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# and a logits layer -- let's say we have 3 classes\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "vgg16_model = Model(inputs=base_vgg16.input, outputs=predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p5YDj2VhrQ6",
        "colab_type": "code",
        "outputId": "75f98c0a-9c8a-440e-9532-da1c29090230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vgg16_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 15,504,195\n",
            "Trainable params: 15,504,195\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD3lxG4IgXFY",
        "colab_type": "code",
        "outputId": "c283b3ec-ef5c-4149-97f3-40e753b5c23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "vgg16_model.trainable = True\n",
        "set_trainable = False\n",
        "for layer in vgg16_model.layers:\n",
        "    if layer.name == 'block4_conv1':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "for layer in vgg16_model.layers:\n",
        "  print(layer.name,' Trainable =',layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_1  Trainable = False\n",
            "block1_conv1  Trainable = False\n",
            "block1_conv2  Trainable = False\n",
            "block1_pool  Trainable = False\n",
            "block2_conv1  Trainable = False\n",
            "block2_conv2  Trainable = False\n",
            "block2_pool  Trainable = False\n",
            "block3_conv1  Trainable = False\n",
            "block3_conv2  Trainable = False\n",
            "block3_conv3  Trainable = False\n",
            "block3_pool  Trainable = False\n",
            "block4_conv1  Trainable = True\n",
            "block4_conv2  Trainable = True\n",
            "block4_conv3  Trainable = True\n",
            "block4_pool  Trainable = True\n",
            "block5_conv1  Trainable = True\n",
            "block5_conv2  Trainable = True\n",
            "block5_conv3  Trainable = True\n",
            "block5_pool  Trainable = True\n",
            "global_average_pooling2d_1  Trainable = True\n",
            "dense_4  Trainable = True\n",
            "dropout_3  Trainable = True\n",
            "dense_5  Trainable = True\n",
            "dropout_4  Trainable = True\n",
            "dense_6  Trainable = True\n",
            "dropout_5  Trainable = True\n",
            "dense_7  Trainable = True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYA-DLu5hm3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.RMSprop(lr=1e-4)\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "vgg16_model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ab7e9a46-4ee3-42fb-b018-a46995690f86",
        "id": "a8bWlvaDiCcz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "train_steps = int(len(df)/batch_size) #total trains set / batch_size\n",
        "val_steps = int(len(df_val)/batch_size)\n",
        "#epochs = 1\n",
        "\n",
        "print('train steps:',train_steps)\n",
        "print('val steps:',val_steps)\n",
        "epochs = 20\n",
        "\n",
        "es = EarlyStopping(monitor='accuracy', mode='max', verbose=1, patience=15)\n",
        "\n",
        "vgg_model_filepath = 'vgg16.h5'\n",
        "mcp = ModelCheckpoint(vgg_model_filepath, monitor='accuracy', save_best_only=True, mode='max')\n",
        "\n",
        "### Train the model with validation \n",
        "vgg16_history = vgg16_model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = 10,\n",
        "                   callbacks=[es, mcp])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train steps: 93\n",
            "val steps: 14\n",
            "Train for 93 steps, validate for 10 steps\n",
            "Epoch 1/20\n",
            "93/93 [==============================] - 114s 1s/step - loss: 0.2662 - accuracy: 0.8774 - val_loss: 1.0066 - val_accuracy: 0.7898\n",
            "Epoch 2/20\n",
            "93/93 [==============================] - 100s 1s/step - loss: 0.2565 - accuracy: 0.8777 - val_loss: 0.7446 - val_accuracy: 0.7852\n",
            "Epoch 3/20\n",
            "93/93 [==============================] - 112s 1s/step - loss: 0.2599 - accuracy: 0.8801 - val_loss: 0.7436 - val_accuracy: 0.8047\n",
            "Epoch 4/20\n",
            "93/93 [==============================] - 102s 1s/step - loss: 0.2481 - accuracy: 0.8825 - val_loss: 0.6192 - val_accuracy: 0.8047\n",
            "Epoch 5/20\n",
            "93/93 [==============================] - 109s 1s/step - loss: 0.2632 - accuracy: 0.8875 - val_loss: 0.5736 - val_accuracy: 0.8016\n",
            "Epoch 6/20\n",
            "93/93 [==============================] - 104s 1s/step - loss: 0.2265 - accuracy: 0.8947 - val_loss: 0.9120 - val_accuracy: 0.7984\n",
            "Epoch 7/20\n",
            "93/93 [==============================] - 99s 1s/step - loss: 0.2581 - accuracy: 0.8909 - val_loss: 0.7569 - val_accuracy: 0.8070\n",
            "Epoch 8/20\n",
            "93/93 [==============================] - 112s 1s/step - loss: 0.2395 - accuracy: 0.8905 - val_loss: 0.7012 - val_accuracy: 0.7828\n",
            "Epoch 9/20\n",
            "93/93 [==============================] - 106s 1s/step - loss: 0.2133 - accuracy: 0.9043 - val_loss: 1.0077 - val_accuracy: 0.7906\n",
            "Epoch 10/20\n",
            "93/93 [==============================] - 112s 1s/step - loss: 0.2655 - accuracy: 0.8953 - val_loss: 0.6851 - val_accuracy: 0.7898\n",
            "Epoch 11/20\n",
            "93/93 [==============================] - 104s 1s/step - loss: 0.2168 - accuracy: 0.9059 - val_loss: 0.6415 - val_accuracy: 0.7883\n",
            "Epoch 12/20\n",
            "93/93 [==============================] - 108s 1s/step - loss: 0.2126 - accuracy: 0.9052 - val_loss: 1.0437 - val_accuracy: 0.7750\n",
            "Epoch 13/20\n",
            "93/93 [==============================] - 106s 1s/step - loss: 0.2187 - accuracy: 0.9063 - val_loss: 1.0739 - val_accuracy: 0.7719\n",
            "Epoch 14/20\n",
            "93/93 [==============================] - 100s 1s/step - loss: 0.2215 - accuracy: 0.9064 - val_loss: 0.7403 - val_accuracy: 0.7742\n",
            "Epoch 15/20\n",
            "93/93 [==============================] - 112s 1s/step - loss: 0.2387 - accuracy: 0.9068 - val_loss: 0.8679 - val_accuracy: 0.7836\n",
            "Epoch 16/20\n",
            "93/93 [==============================] - 102s 1s/step - loss: 0.1869 - accuracy: 0.9157 - val_loss: 1.1879 - val_accuracy: 0.7969\n",
            "Epoch 17/20\n",
            "93/93 [==============================] - 110s 1s/step - loss: 0.1765 - accuracy: 0.9215 - val_loss: 1.4610 - val_accuracy: 0.7922\n",
            "Epoch 18/20\n",
            "93/93 [==============================] - 103s 1s/step - loss: 0.1819 - accuracy: 0.9183 - val_loss: 1.0139 - val_accuracy: 0.7992\n",
            "Epoch 19/20\n",
            "93/93 [==============================] - 99s 1s/step - loss: 0.1865 - accuracy: 0.9197 - val_loss: 0.9846 - val_accuracy: 0.7875\n",
            "Epoch 20/20\n",
            "93/93 [==============================] - 114s 1s/step - loss: 0.1751 - accuracy: 0.9262 - val_loss: 1.0190 - val_accuracy: 0.7898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d2ad3eb1-6919-4832-e124-549a0735dd73",
        "id": "9sHqtlA1gcsM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "vgg16_model = load_model(vgg_model_filepath)\n",
        "test_steps = int(len(df_test)/batch_size)\n",
        "\n",
        "metrics = vgg16_model.evaluate(test_dataset,\n",
        "                   steps = test_steps)\n",
        "print(\"model accuracy:\",metrics[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 1030s 74s/step - loss: 1.1920 - accuracy: 0.7746\n",
            "model accuracy: 0.7745536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcNPUHwyg2qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}